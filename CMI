{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":673.251127,"end_time":"2024-12-12T20:25:29.287784","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-12T20:14:16.036657","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The baseline was taken from [CMI | Reproducible results |FixSeed,LGB-CPU|LB.494](https://www.kaggle.com/code/kuosys/cmi-reproducible-results-fixseed-lgb-cpu-lb-494) 🙏","metadata":{"papermill":{"duration":0.007407,"end_time":"2024-12-12T20:14:18.711807","exception":false,"start_time":"2024-12-12T20:14:18.7044","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Metric Exploration","metadata":{"papermill":{"duration":0.00634,"end_time":"2024-12-12T20:14:18.724975","exception":false,"start_time":"2024-12-12T20:14:18.718635","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nTo compute the quadratic weighted kappa, we construct three matrices, $O$, $W$, and $E$, with $N$ the number of distinct labels.\n\nThe matrix $O$ is an $N × 𝑁$ histogram matrix such that $O_{i,j}$ corresponds to the number of instances that have an actual value $i$ and a predicted value $j$.\n\nThe matrix $W$ is an $N × 𝑁$ matrix of weights, calculated based on the squared difference between actual and predicted values:\n\n$$ W_{i,j} = \\frac{(i - j)^2}{(N - 1)^2}.$$\n\nThe matrix $E$ is an $N × 𝑁$ histogram matrix of expected outcomes, calculated assuming that there is no correlation between values. This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that $E$ and $O$ have the same sum.\n\nFrom these three matrices, the quadratic weighted kappa is calculated as: \n\n$$𝜅 = 1 - \\frac{\\sum_{i, j} W_{i, j} \\cdot O_{i, j}}{\\sum_{i, j} W_{i, j} \\cdot E_{i, j}}.$$\n\nThis metric can be interpreted as one minus the ratio of the squared error on the predicted values ​​to the error if we had randomly assigned predictions from a class distribution given by the proportions of predicted values. To learn more about this metric, you can find detailed information online, for example at [https://datatab.net/tutorial/weighted-cohens-kappa](https://datatab.net/tutorial/weighted-cohens-kappa).","metadata":{"papermill":{"duration":0.006269,"end_time":"2024-12-12T20:14:18.738524","exception":false,"start_time":"2024-12-12T20:14:18.732255","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport copy\nimport pickle\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\n\nimport plotly.express as px\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"execution":{"iopub.status.busy":"2024-12-13T13:07:21.927397Z","iopub.execute_input":"2024-12-13T13:07:21.927656Z","iopub.status.idle":"2024-12-13T13:07:40.683794Z","shell.execute_reply.started":"2024-12-13T13:07:21.927629Z","shell.execute_reply":"2024-12-13T13:07:40.682795Z"},"papermill":{"duration":23.622575,"end_time":"2024-12-12T20:14:42.367516","exception":false,"start_time":"2024-12-12T20:14:18.744941","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')#3960 rows × 82 columns - 2736 rows × 155 columns\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')#(20, 59) - (20, 154)\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')#id/sii\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))#43330 rows × 13 columns\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]#1次元配列に変換,indexes\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)#['id=00115b9f', 'id=001f3379']\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n        \ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")#996 rows × 97 columns\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")#(2, 97)\n\ntime_series_cols = train_ts.columns.tolist()#リストか\ntime_series_cols.remove(\"id\")#id削除\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)   \n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')#欠損値を明示的に扱う\n        df[c] = df[c].astype('category')#カテゴリカル型に変換\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\n#カテゴリカル変数の各値にユニークな整数値を割り当てるためのマッピングを作成\ndef create_mapping(column, dataset):\n    #指定したカラム (column) のユニークな値を取得し、unique_values に格納します。\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}# 辞書を作成\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    #DataFrame の col カラムの値を、mapping 辞書に従って整数値に置き換えます。\n    #その後、データ型を整数型 (int) に変換\n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\n#順序尺度（例えば、評価が「良い」「普通」「悪い」のように順序付けられている尺度）の評価においてよく利用\n#weights='quadratic': カッパ係数の重み付け方式を指定します。\n#'quadratic' は、ラベル間の差が大きいほどペナルティが大きくなる二次的な重み付け方式を意味します。\n#クアドラティックウェイトカッパ係数は、2つの評価者（またはモデル）が同じ対象に対して行った評価の一致度を測る指標\n#値の範囲: -1 から 1 の間の値を取ります。\n#1: 完全一致\n#0: 偶然の一致\n#-1: 完全な不一致\n#クアドラティックウェイトカッパ係数を計算する関\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n#予測値を指定された閾値に基づいて丸める関数\n#丸める前の予測値 (oof_non_rounded) と、閾値のリスト (thresholds) を受け取ります。\n#np.whereを使って、丸める後の予測値を返します。\n#予測値が最小の閾値未満なら 0\n#予測値が最小の閾値と次の閾値の間なら 1\n##... (同様)\n#最も大きい閾値を超えるなら 3\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\n#予測値の丸め具合 (閾値) に応じた QWK スコアを計算する関数\n#丸め後の予測値と真のラベルを使って、QWK スコアを計算し、負の値で返します。 (最小化を目指すため)\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n#機械学習モデルを訓練し、閾値を最適化する関数\n#evaluate_predictions 関数を使って、閾値を最適化します。\n#最適化手法は Nelder-Mead 法が使用されています。\n#最適化された閾値が得られます。\n#最適化された閾値を使って、検証データに対する丸め後の予測値 (oof_tuned) と、全体の QWK スコア (tKappa) を計算し、\n#表示\n#テストデータに対する予測値 (tpm) を各 Fold の平均で取り、最適化された閾値を使って丸めます。\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) #: 丸める前の予測値を格納する配列\n    oof_rounded = np.zeros(len(y), dtype=int) # 丸めた後の予測値を格納する配列\n    test_preds = np.zeros((len(test_data), n_splits))#テストデータに対する各 Fold での予測値を格納する配列\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n    optimized_thresholds = KappaOPtimizer.x\n    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds","metadata":{"execution":{"iopub.status.busy":"2024-12-13T13:07:40.685596Z","iopub.execute_input":"2024-12-13T13:07:40.686325Z","iopub.status.idle":"2024-12-13T13:08:53.001521Z","shell.execute_reply.started":"2024-12-13T13:07:40.686296Z","shell.execute_reply":"2024-12-13T13:08:53.000639Z"},"papermill":{"duration":73.652495,"end_time":"2024-12-12T20:15:56.027264","exception":false,"start_time":"2024-12-12T20:14:42.374769","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [01:11<00:00, 13.85it/s]\n100%|██████████| 2/2 [00:00<00:00, 10.66it/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#改善点１：ベイス最適化。２：depthを−１にする\n\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')#2736 rows × 155 column\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')#(20, 154)\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')#id/sii\n\n#for process_file\ndirname=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\nfilename='id=00115b9f'\ndf = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\ndf\ndf.drop('step', axis=1, inplace=True)\ndf\nx = df.describe().values#(8,12)\nx.reshape(-1)#(96,)\nfilename.split(\"=\")[1]#'00115b9f'\n\n#for load_time_series\ndirname = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\nids  = os.listdir(dirname)#['id=00115b9f', 'id=001f3379']\nwith ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\nstats, indexes = zip(*results)\nindexes#('00115b9f', '001f3379')\ndf = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\ndf['id'] = indexes\ndf\n\n#under\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)#(3960, 177)\ntest = test.drop('id', axis=1)#(20, 154)\n#print(test.shape)\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols#59+96=155\n\ntrain = train[featuresCols]#3960 rows × 155 columns\ntrain = train.dropna(subset='sii')#'sii' に欠損値 (NaN) が含まれる行を削除 2736 rows × 155 columns\n\n#categorical列への処理\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')#欠損値を明示的に扱う\n        df[c] = df[c].astype('category')#カテゴリカル型に変換\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\n#カテゴリカル変数の各値にユニークな整数値を割り当てるためのマッピングを作成\ndef create_mapping(column, dataset):\n    #指定したカラム (column) のユニークな値を取得し、unique_values に格納します。\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}# 辞書を作成\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    #DataFrame の col カラムの値を、mapping 辞書に従って整数値に置き換えます。\n    #その後、データ型を整数型 (int) に変換\n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\n#順序尺度（例えば、評価が「良い」「普通」「悪い」のように順序付けられている尺度）の評価においてよく利用\n#weights='quadratic': カッパ係数の重み付け方式を指定します。\n#'quadratic' は、ラベル間の差が大きいほどペナルティが大きくなる二次的な重み付け方式を意味します。\n#クアドラティックウェイトカッパ係数は、2つの評価者（またはモデル）が同じ対象に対して行った評価の一致度を測る指標\n#値の範囲: -1 から 1 の間の値を取ります。\n#1: 完全一致\n#0: 偶然の一致\n#-1: 完全な不一致\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n#予測確率 (oof_non_rounded) を、与えられた3つのリスト閾値 (thresholds) に基づいて、4つのクラス (0, 1, 2, 3) に分類する関数\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n# 与えられた閾値 (thresholds) に基づいて、予測確率 (oof_non_rounded) をクラスラベルに分類し、\n#その結果と真のラベル (y_true) を比較して、クアドラティックウェイトカッパ (QWK) スコアを計算する関数\n#最小化問題を解くため、QWK スコアの値にマイナスを掛けて返します\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)#クラスラベルに変換\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n#for trainML\nn_splits=5\nSEED=42\nmodel_class = XGBRegressor(\n    learning_rate=0.05,\n    max_depth=6,\n    n_estimators=200,\n    subsample=0.8,\n    colsample_bytree = 0.8,\n    reg_alpha=1,\n    reg_lambda=5,\n    random_state=SEED\n)\n\nX = train.drop(['sii'], axis=1)\ny = train['sii']\n\nSKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \ntrain_S = []\ntest_S = []\n    \noof_non_rounded = np.zeros(len(y), dtype=float) #(2736,)\noof_rounded = np.zeros(len(y), dtype=int) \ntest_preds = np.zeros((len(test), n_splits))#(20, 5), array([[0., 0., 0., 0., 0.],,,,\n\nfor fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred#max:2.1903676986694336,min:-0.1739702671766281\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))#0.8930017158829445\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)#0.3609061783551819\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        #clear_output(wait=True)#出力が蓄積されず、常に最新の Fold の情報が表示される\nprint(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\nprint(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n#閾値の最適化\n#minimize 関数: minimize 関数を使用して、evaluate_predictions 関数を最小化\n#x0=[0.5, 1.5, 2.5]: 最適化の初期値として、閾値のリストを指定\n#args=(y, oof_non_rounded): evaluate_predictions 関数に渡す引数 (y_true, oof_non_rounded) を指定\n#method='Nelder-Mead': 最適化アルゴリズムとして、Nelder-Mead 法を指定\n#このコードの目的:\n#1.予測確率を適切なクラスラベルに変換するための最適な閾値を決定します。\n#2.QWK スコアを最大化することで、予測の精度を向上させます。\n#補足:Nelder-Mead 法は、微分不可能な関数に対しても適用できるため、QWK スコアのような複雑な評価関数の最適化に適している\nKappaOPtimizer = minimize(evaluate_predictions,\n                            x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                            method='Nelder-Mead')\nassert KappaOPtimizer.success, \"Optimization did not converge.\"#収束したかどうかを示すフラグ\nprint('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)#最適化された閾値のリスト\n\noof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\ntKappa = quadratic_weighted_kappa(y, oof_tuned)\n\nprint(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\ntpm = test_preds.mean(axis=1)#(20, 5) - (20,)\ntpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \nsubmission = pd.DataFrame({\n    'id': sample['id'],\n    'sii': tpTuned\n})\noptimized_thresholds = KappaOPtimizer.x\n#return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T17:09:28.724798Z","iopub.execute_input":"2024-12-13T17:09:28.725122Z","iopub.status.idle":"2024-12-13T17:09:50.051887Z","shell.execute_reply.started":"2024-12-13T17:09:28.725096Z","shell.execute_reply":"2024-12-13T17:09:50.051052Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 11.92it/s]\nTraining Folds:  20%|██        | 1/5 [00:04<00:16,  4.18s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Train QWK: 0.8855, Validation QWK: 0.4055\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  40%|████      | 2/5 [00:08<00:12,  4.13s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Train QWK: 0.8926, Validation QWK: 0.4399\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  60%|██████    | 3/5 [00:12<00:08,  4.17s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Train QWK: 0.8957, Validation QWK: 0.4328\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  80%|████████  | 4/5 [00:16<00:04,  4.17s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Train QWK: 0.9027, Validation QWK: 0.3094\n","output_type":"stream"},{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:20<00:00,  4.15s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Train QWK: 0.8930, Validation QWK: 0.3609\nMean Train QWK --> 0.8939\nMean Validation QWK ---> 0.3897\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.6264773  0.89171596 2.75349008]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.445\u001b[0m\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"SEED = 42\nn_splits = 5\n\nmodel = XGBRegressor(\n    learning_rate=0.05,\n    max_depth=6,\n    n_estimators=200,\n    subsample=0.8,\n    colsample_bytree = 0.8,\n    reg_alpha=1,\n    reg_lambda=5,\n    random_state=SEED\n)\n\n# we get out of fold predictions for further exploration\nsubmission, y_pred, y_pred_non_rounded, y_true, optimized_thresholds = TrainML(model, test)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T15:47:47.180464Z","iopub.execute_input":"2024-12-13T15:47:47.180826Z","iopub.status.idle":"2024-12-13T15:48:08.957236Z","shell.execute_reply.started":"2024-12-13T15:47:47.180796Z","shell.execute_reply":"2024-12-13T15:48:08.956344Z"},"papermill":{"duration":21.062077,"end_time":"2024-12-12T20:16:17.110347","exception":false,"start_time":"2024-12-12T20:15:56.04827","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:21<00:00,  4.30s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.8939\nMean Validation QWK ---> 0.3897\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.6264773  0.89171596 2.75349008]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.445\u001b[0m\n","output_type":"stream"}],"execution_count":90},{"cell_type":"markdown","source":"Training Folds: 100%|██████████| 5/5 [00:21<00:00,  4.30s/it]\nMean Train QWK --> 0.8939\nMean Validation QWK ---> 0.3897\n\nOPTIMIZED THRESHOLDS [0.6264773  0.89171596 2.75349008]\n----> || Optimized QWK SCORE ::  0.445","metadata":{}},{"cell_type":"markdown","source":"Next, we simulate changes in the scores by adding one more observation (with classes 0, 1, 2, 3 specified by `y_new`) and calculate how much better/worse the metric becomes at different values of the predictions `pred_new` compared to the prediction 0.","metadata":{"papermill":{"duration":0.020364,"end_time":"2024-12-12T20:16:17.151975","exception":false,"start_time":"2024-12-12T20:16:17.131611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#与えられた予測結果 (y_pred) に対して、1つの新しい予測値 (pred_new) を追加した場合の\n#クアドラティックウェイトカッパ (QWK) スコアの変化を計算し、その結果を DataFrame にまとめた\ndf_score_changes = []\nfor y_new in range(4):\n    item = {'y_new': y_new}\n    score_pred_zero = quadratic_weighted_kappa(list(y_true) + [y_new], list(y_pred) + [0])\n    for pred_new in range(4):\n        score = quadratic_weighted_kappa(list(y_true) + [y_new], list(y_pred) + [pred_new])\n        item[f'pred_new={pred_new}'] = score - score_pred_zero\n    df_score_changes.append(item)\n\ndf_score_changes = pd.DataFrame(df_score_changes)\ndf_score_changes","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:30:38.136128Z","iopub.execute_input":"2024-12-13T17:30:38.136479Z","iopub.status.idle":"2024-12-13T17:30:38.214205Z","shell.execute_reply.started":"2024-12-13T17:30:38.136448Z","shell.execute_reply":"2024-12-13T17:30:38.213411Z"},"papermill":{"duration":0.101721,"end_time":"2024-12-12T20:16:17.27411","exception":false,"start_time":"2024-12-12T20:16:17.172389","status":"completed"},"tags":[],"trusted":true},"outputs":[{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"   y_new  pred_new=0  pred_new=1  pred_new=2  pred_new=3\n0      0         0.0   -0.000313   -0.000880   -0.001701\n1      1         0.0    0.000261    0.000267    0.000018\n2      2         0.0    0.000835    0.001415    0.001738\n3      3         0.0    0.001407    0.002559    0.003454","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_new</th>\n      <th>pred_new=0</th>\n      <th>pred_new=1</th>\n      <th>pred_new=2</th>\n      <th>pred_new=3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-0.000313</td>\n      <td>-0.000880</td>\n      <td>-0.001701</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.000261</td>\n      <td>0.000267</td>\n      <td>0.000018</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.000835</td>\n      <td>0.001415</td>\n      <td>0.001738</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.001407</td>\n      <td>0.002559</td>\n      <td>0.003454</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":150},{"cell_type":"markdown","source":"Our analysis reveals that when `y_new=2`, predicting class 3 results in a higher score than predicting the true class 2 for a one addtional observation 🤯","metadata":{"papermill":{"duration":0.0204,"end_time":"2024-12-12T20:16:17.315784","exception":false,"start_time":"2024-12-12T20:16:17.295384","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Let's plot the change in the metric as we vary the threshold (while keeping the others fixed).","metadata":{"papermill":{"duration":0.020529,"end_time":"2024-12-12T20:16:17.35683","exception":false,"start_time":"2024-12-12T20:16:17.336301","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#最適化された閾値 (optimized_thresholds) を可視化するためのものです。\n#特に、各閾値の変化が QWK スコアに与える影響 を視覚的に確認する\nfor t_idx in range(3):\n    df_plot = []\n    for t in np.arange(0.0, 3.0, 0.001):\n        thresholds = copy.copy(optimized_thresholds)\n        thresholds[t_idx] = t\n        score = -evaluate_predictions(thresholds, y_true, y_pred_non_rounded)\n        df_plot.append({f't_{t_idx}': t, 'score': score})\n    \n    df_plot = pd.DataFrame(df_plot)\n    fig = px.line(df_plot, x=f't_{t_idx}', y='score', title=f't_{t_idx}')\n    fig.show(renderer='iframe')","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:40:17.467893Z","iopub.execute_input":"2024-12-13T17:40:17.468556Z","iopub.status.idle":"2024-12-13T17:40:40.681803Z","shell.execute_reply.started":"2024-12-13T17:40:17.468523Z","shell.execute_reply":"2024-12-13T17:40:40.680979Z"},"papermill":{"duration":22.70751,"end_time":"2024-12-12T20:16:40.084859","exception":false,"start_time":"2024-12-12T20:16:17.377349","status":"completed"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_151.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_151.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"545px\"\n    src=\"iframe_figures/figure_151.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":151},{"cell_type":"code","source":"# The threshold optimizer in the code appears to be finding a local maximum, not the global maximum\nprint('optimized_thresholds score:', -evaluate_predictions(optimized_thresholds, y_true, y_pred_non_rounded))\nprint('another thresholds score:', -evaluate_predictions([0.6264773 , 0.89171596, 1.64], y_true, y_pred_non_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:41:19.191580Z","iopub.execute_input":"2024-12-13T17:41:19.192437Z","iopub.status.idle":"2024-12-13T17:41:19.203841Z","shell.execute_reply.started":"2024-12-13T17:41:19.192401Z","shell.execute_reply":"2024-12-13T17:41:19.202980Z"},"papermill":{"duration":0.033475,"end_time":"2024-12-12T20:16:40.139948","exception":false,"start_time":"2024-12-12T20:16:40.106473","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"optimized_thresholds score: 0.4447091772741697\nanother thresholds score: 0.4528701027295803\n","output_type":"stream"}],"execution_count":152},{"cell_type":"markdown","source":"# Issues with the Quadratic Weighted Kappa metric\n\n* **Non-intuitive behavior:** It's been shown that predicting incorrect values can sometimes result in a better QWK score than predicting the actual values. This highlights the complexity of interpreting changes in the metric and can make it difficult to understand whether model improvements are truly meaningful.\n* **Loss of information due to discretization:** QWK requires discrete predictions, which means continuous model outputs need to be thresholded into distinct categories. This process can obscure a significant amount of information about the model's performance. For example, QWK doesn't consider how well items are ranked within each predicted class. Items on the edge of thresholds and those deep inside a category are treated equally, potentially masking important distinctions. In my opinion, a continuous metric might provide a more nuanced and informative assessment of model quality, potentially leading to better business decisions.","metadata":{"papermill":{"duration":0.020504,"end_time":"2024-12-12T20:16:40.18136","exception":false,"start_time":"2024-12-12T20:16:40.160856","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# BASELINE","metadata":{"papermill":{"duration":0.020535,"end_time":"2024-12-12T20:16:40.222671","exception":false,"start_time":"2024-12-12T20:16:40.202136","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Issues with the following baseline (as well as in similar top-performing models on a public dataset)\n* Autoencoder for `train` and `test` dataset is fitted separately and it might lead to severely different encoding for the same data (see using `perform_autoencoder`)\n* Thresholds optimizer finds only local extremum (see using `KappaOPtimizer = minimize(...)`)\n\nIncredibly, this dubious technique gives a good score on the public leaderboard 🤷. Let's hope that for private dataset something more correct will work better 🙏.","metadata":{"papermill":{"duration":0.020695,"end_time":"2024-12-12T20:16:40.264012","exception":false,"start_time":"2024-12-12T20:16:40.243317","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:51:34.623805Z","iopub.execute_input":"2024-12-13T17:51:34.624473Z","iopub.status.idle":"2024-12-13T17:52:15.903411Z","shell.execute_reply.started":"2024-12-13T17:51:34.624440Z","shell.execute_reply":"2024-12-13T17:52:15.902133Z"},"papermill":{"duration":40.674512,"end_time":"2024-12-12T20:17:20.959469","exception":false,"start_time":"2024-12-12T20:16:40.284957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":153},{"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetRegressor\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:52:15.905460Z","iopub.execute_input":"2024-12-13T17:52:15.905768Z","iopub.status.idle":"2024-12-13T17:52:15.924664Z","shell.execute_reply.started":"2024-12-13T17:52:15.905738Z","shell.execute_reply":"2024-12-13T17:52:15.924067Z"},"papermill":{"duration":0.039431,"end_time":"2024-12-12T20:17:21.020376","exception":false,"start_time":"2024-12-12T20:17:20.980945","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":154},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:52:15.925636Z","iopub.execute_input":"2024-12-13T17:52:15.925887Z","iopub.status.idle":"2024-12-13T17:52:15.932699Z","shell.execute_reply.started":"2024-12-13T17:52:15.925863Z","shell.execute_reply":"2024-12-13T17:52:15.931846Z"},"papermill":{"duration":0.029756,"end_time":"2024-12-12T20:17:21.071019","exception":false,"start_time":"2024-12-12T20:17:21.041263","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":155},{"cell_type":"code","source":"import random\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(2024)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:52:15.934384Z","iopub.execute_input":"2024-12-13T17:52:15.934949Z","iopub.status.idle":"2024-12-13T17:52:15.946977Z","shell.execute_reply.started":"2024-12-13T17:52:15.934889Z","shell.execute_reply":"2024-12-13T17:52:15.946289Z"},"papermill":{"duration":0.03252,"end_time":"2024-12-12T20:17:21.125269","exception":false,"start_time":"2024-12-12T20:17:21.092749","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":156},{"cell_type":"code","source":"SEED = 42\nn_splits = 5","metadata":{"execution":{"iopub.status.busy":"2024-12-13T17:52:15.948078Z","iopub.execute_input":"2024-12-13T17:52:15.948639Z","iopub.status.idle":"2024-12-13T17:52:15.954644Z","shell.execute_reply.started":"2024-12-13T17:52:15.948602Z","shell.execute_reply":"2024-12-13T17:52:15.953995Z"},"papermill":{"duration":0.0261,"end_time":"2024-12-12T20:17:21.172107","exception":false,"start_time":"2024-12-12T20:17:21.146007","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":157},{"cell_type":"markdown","source":"# Feature Engineering\n\n- **Feature Selection**: The dataset contains features related to physical characteristics (e.g., BMI, Height, Weight), behavioral aspects (e.g., internet usage), and fitness data (e.g., endurance time). \n- **Categorical Feature Encoding**: Categorical features are mapped to numerical values using custom mappings for each unique category within the dataset. This ensures compatibility with machine learning algorithms that require numerical input.\n- **Time Series Aggregation**: Time series statistics (e.g., mean, standard deviation) from the actigraphy data are computed and merged into the main dataset to create additional features for model training.\n","metadata":{"papermill":{"duration":0.020578,"end_time":"2024-12-12T20:17:21.213535","exception":false,"start_time":"2024-12-12T20:17:21.192957","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, input_dim*2),\n            nn.ReLU(),\n            nn.Linear(input_dim*2, input_dim*3),\n            nn.ReLU(),\n            nn.Linear(input_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded\n\ndef feature_engineering(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n    \n    return df","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:17:21.256383Z","iopub.status.busy":"2024-12-12T20:17:21.256128Z","iopub.status.idle":"2024-12-12T20:17:21.270041Z","shell.execute_reply":"2024-12-12T20:17:21.269467Z"},"papermill":{"duration":0.037195,"end_time":"2024-12-12T20:17:21.271465","exception":false,"start_time":"2024-12-12T20:17:21.23427","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntime_series_cols = train_ts_encoded.columns.tolist()\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]\n        \ntrain = train_imputed\n\ntrain = feature_engineering(train)\ntrain = train.dropna(thresh=10, axis=0)\ntest = feature_engineering(test)\n\ntrain = train.drop('id', axis=1)\ntest  = test .drop('id', axis=1)   \n\n\nfeaturesCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\nfeaturesCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n\nfeaturesCols += time_series_cols\ntest = test[featuresCols]","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:17:21.314446Z","iopub.status.busy":"2024-12-12T20:17:21.314199Z","iopub.status.idle":"2024-12-12T20:18:48.1865Z","shell.execute_reply":"2024-12-12T20:18:48.185786Z"},"papermill":{"duration":86.896067,"end_time":"2024-12-12T20:18:48.188603","exception":false,"start_time":"2024-12-12T20:17:21.292536","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if np.any(np.isinf(train)):\n    train = train.replace([np.inf, -np.inf], np.nan)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:48.265651Z","iopub.status.busy":"2024-12-12T20:18:48.264419Z","iopub.status.idle":"2024-12-12T20:18:48.27441Z","shell.execute_reply":"2024-12-12T20:18:48.273533Z"},"papermill":{"duration":0.048881,"end_time":"2024-12-12T20:18:48.276056","exception":false,"start_time":"2024-12-12T20:18:48.227175","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training and Evaluation\n\n- **Model Types**: Various models are used, including:\n  - **LightGBM**: A gradient-boosting framework known for its speed and efficiency with large datasets.\n  - **XGBoost**: Another powerful gradient-boosting model used for structured data.\n  - **CatBoost**: Optimized for categorical features without the need for extensive preprocessing.\n  - **Voting Regressor**: An ensemble model that combines the predictions of LightGBM, XGBoost, and CatBoost for better accuracy.\n- **Cross-Validation**: Stratified K-Folds cross-validation is employed to split the data into training and validation sets, ensuring balanced class distribution in each fold.\n- **Quadratic Weighted Kappa (QWK)**: The performance of the models is evaluated using QWK, which measures the agreement between predicted and actual values, taking into account the ordinal nature of the target variable.\n- **Threshold Optimization**: The `minimize` function from `scipy.optimize` is used to fine-tune decision thresholds that map continuous predictions to discrete categories (None, Mild, Moderate, Severe).\n","metadata":{"papermill":{"duration":0.035922,"end_time":"2024-12-12T20:18:48.348136","exception":false,"start_time":"2024-12-12T20:18:48.312214","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:48.421854Z","iopub.status.busy":"2024-12-12T20:18:48.421168Z","iopub.status.idle":"2024-12-12T20:18:48.430396Z","shell.execute_reply":"2024-12-12T20:18:48.429715Z"},"papermill":{"duration":0.047855,"end_time":"2024-12-12T20:18:48.432006","exception":false,"start_time":"2024-12-12T20:18:48.384151","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Hyperparameter Tuning\n\n- **LightGBM Parameters**: Hyperparameters such as `learning_rate`, `max_depth`, `num_leaves`, and `feature_fraction` are tuned to improve the performance of the LightGBM model. These parameters control the complexity of the model and its ability to generalize to new data.\n- **XGBoost and CatBoost Parameters**: Similar tuning is applied for XGBoost and CatBoost, adjusting parameters such as `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `regularization` terms (`reg_alpha`, `reg_lambda`). These help in controlling overfitting and ensuring the model's robustness.","metadata":{"papermill":{"duration":0.035531,"end_time":"2024-12-12T20:18:48.503391","exception":false,"start_time":"2024-12-12T20:18:48.46786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01,  # Increased from 2.68e-06\n    'device': 'cpu'\n\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED,\n    'tree_method': 'gpu_hist',\n\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10,  # Increase this value\n    'task_type': 'GPU'\n\n}","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:48.575887Z","iopub.status.busy":"2024-12-12T20:18:48.575655Z","iopub.status.idle":"2024-12-12T20:18:48.580505Z","shell.execute_reply":"2024-12-12T20:18:48.579855Z"},"papermill":{"duration":0.042924,"end_time":"2024-12-12T20:18:48.581996","exception":false,"start_time":"2024-12-12T20:18:48.539072","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# New: TabNet\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_tabnet.callbacks import Callback\nimport os\nimport torch\nfrom pytorch_tabnet.callbacks import Callback\n\nclass TabNetWrapper(BaseEstimator, RegressorMixin):\n    def __init__(self, **kwargs):\n        self.model = TabNetRegressor(**kwargs)\n        self.kwargs = kwargs\n        self.imputer = SimpleImputer(strategy='median')\n        self.best_model_path = 'best_tabnet_model.pt'\n        \n    def fit(self, X, y):\n        # Handle missing values\n        X_imputed = self.imputer.fit_transform(X)\n        \n        if hasattr(y, 'values'):\n            y = y.values\n            \n        # Create internal validation set\n        X_train, X_valid, y_train, y_valid = train_test_split(\n            X_imputed, \n            y, \n            test_size=0.2,\n            random_state=42\n        )\n        \n        # Train TabNet model\n        history = self.model.fit(\n            X_train=X_train,\n            y_train=y_train.reshape(-1, 1),\n            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n            eval_name=['valid'],\n            eval_metric=['mse'],\n            max_epochs=200,\n            patience=20,\n            batch_size=1024,\n            virtual_batch_size=128,\n            num_workers=0,\n            drop_last=False,\n            callbacks=[\n                TabNetPretrainedModelCheckpoint(\n                    filepath=self.best_model_path,\n                    monitor='valid_mse',\n                    mode='min',\n                    save_best_only=True,\n                    verbose=True\n                )\n            ]\n        )\n        \n        # Load the best model\n        if os.path.exists(self.best_model_path):\n            self.model.load_model(self.best_model_path)\n            os.remove(self.best_model_path)  # Remove temporary file\n        \n        return self\n    \n    def predict(self, X):\n        X_imputed = self.imputer.transform(X)\n        return self.model.predict(X_imputed).flatten()\n    \n    def __deepcopy__(self, memo):\n        # Add deepcopy support for scikit-learn\n        cls = self.__class__\n        result = cls.__new__(cls)\n        memo[id(self)] = result\n        for k, v in self.__dict__.items():\n            setattr(result, k, deepcopy(v, memo))\n        return result\n\n# TabNet hyperparameters\nTabNet_Params = {\n    'n_d': 64,              # Width of the decision prediction layer\n    'n_a': 64,              # Width of the attention embedding for each step\n    'n_steps': 5,           # Number of steps in the architecture\n    'gamma': 1.5,           # Coefficient for feature selection regularization\n    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n    'lambda_sparse': 1e-4,  # Sparsity regularization\n    'optimizer_fn': torch.optim.Adam,\n    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n    'mask_type': 'entmax',\n    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n    'verbose': 1,\n    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n}\n\nclass TabNetPretrainedModelCheckpoint(Callback):\n    def __init__(self, filepath, monitor='val_loss', mode='min', \n                 save_best_only=True, verbose=1):\n        super().__init__()  # Initialize parent class\n        self.filepath = filepath\n        self.monitor = monitor\n        self.mode = mode\n        self.save_best_only = save_best_only\n        self.verbose = verbose\n        self.best = float('inf') if mode == 'min' else -float('inf')\n        \n    def on_train_begin(self, logs=None):\n        self.model = self.trainer  # Use trainer itself as model\n        \n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        current = logs.get(self.monitor)\n        if current is None:\n            return\n        \n        # Check if current metric is better than best\n        if (self.mode == 'min' and current < self.best) or \\\n           (self.mode == 'max' and current > self.best):\n            if self.verbose:\n                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n            self.best = current\n            if self.save_best_only:\n                self.model.save_model(self.filepath)  # Save the entire model","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:48.655405Z","iopub.status.busy":"2024-12-12T20:18:48.655184Z","iopub.status.idle":"2024-12-12T20:18:48.667653Z","shell.execute_reply":"2024-12-12T20:18:48.666988Z"},"papermill":{"duration":0.051218,"end_time":"2024-12-12T20:18:48.669191","exception":false,"start_time":"2024-12-12T20:18:48.617973","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Learning and Submission Preparation\n\n- **Ensemble Learning**: The model uses a **Voting Regressor**, which combines the predictions from LightGBM, XGBoost, and CatBoost. This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.\n- **Out-of-Fold (OOF) Predictions**: During cross-validation, out-of-fold predictions are generated for the training set, which helps in model evaluation without data leakage.\n- **Kappa Optimizer**: The Kappa Optimizer ensures that the predicted values are as close to the actual values as possible by adjusting the thresholds used to convert raw model outputs into class labels.\n- **Test Set Predictions**: After the model is trained and thresholds are optimized, the test dataset is processed, and predictions are generated using the ensemble model. These predictions are converted into the appropriate format for submission.\n- **Submission File Creation**: The predictions are saved in a CSV file following the required format for submission (e.g., for a Kaggle competition), which includes columns like `id` and `sii` (Severity Impairment Index).","metadata":{"papermill":{"duration":0.035821,"end_time":"2024-12-12T20:18:48.741796","exception":false,"start_time":"2024-12-12T20:18:48.705975","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Final Results and Performance Metrics\n\n- **Train and Validation Scores**: After training across multiple folds, the mean Quadratic Weighted Kappa (QWK) score is calculated for both the training and validation datasets, providing an indicator of model performance. \n- **Optimized QWK Score**: The final optimized QWK score after threshold tuning is displayed, showcasing the model's ability to predict the severity levels effectively.\n- **Test Predictions**: The test set predictions are evaluated, and a breakdown of the predicted severity levels (None, Mild, Moderate, Severe) is shown, along with their respective counts.","metadata":{"papermill":{"duration":0.038161,"end_time":"2024-12-12T20:18:48.817502","exception":false,"start_time":"2024-12-12T20:18:48.779341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\nTabNet_Model = TabNetWrapper(**TabNet_Params) # New","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:48.894448Z","iopub.status.busy":"2024-12-12T20:18:48.894156Z","iopub.status.idle":"2024-12-12T20:18:48.905391Z","shell.execute_reply":"2024-12-12T20:18:48.904817Z"},"papermill":{"duration":0.050985,"end_time":"2024-12-12T20:18:48.906901","exception":false,"start_time":"2024-12-12T20:18:48.855916","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **》》》Model1.Train**\n---","metadata":{"papermill":{"duration":0.03675,"end_time":"2024-12-12T20:18:48.979839","exception":false,"start_time":"2024-12-12T20:18:48.943089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"voting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model),\n    ('tabnet', TabNet_Model)\n],weights=[4.0,4.0,5.0,4.0])\n\nSubmission1 = TrainML(voting_model, test)\n\nSubmission1","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:18:49.052639Z","iopub.status.busy":"2024-12-12T20:18:49.052328Z","iopub.status.idle":"2024-12-12T20:20:03.76138Z","shell.execute_reply":"2024-12-12T20:20:03.76038Z"},"papermill":{"duration":74.74773,"end_time":"2024-12-12T20:20:03.763394","exception":false,"start_time":"2024-12-12T20:18:49.015664","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\n],weights=[5.0,4.0,4.0,4.0])\nMean Train QWK --> 0.7424\nMean Validation QWK ---> 0.4735\n----> || Optimized QWK SCORE ::  0.533\n\n```","metadata":{"papermill":{"duration":0.037036,"end_time":"2024-12-12T20:20:03.838467","exception":false,"start_time":"2024-12-12T20:20:03.801431","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---\n# **》》》Model2**\n---","metadata":{"papermill":{"duration":0.036329,"end_time":"2024-12-12T20:20:03.911459","exception":false,"start_time":"2024-12-12T20:20:03.87513","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n        \ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)   \n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.49, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    thresholds = KappaOPtimizer.x\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    fold_weights = [1.25, 1.0, 1.0, 1.0, 1.0]\n    tpm = test_preds.dot(fold_weights) / np.sum(fold_weights)\n    tpTuned = threshold_Rounder(tpm, thresholds)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# Train the ensemble model\nSubmission2 = TrainML(voting_model, test)\n\n# Save submission\n#Submission2.to_csv('submission.csv', index=False)\nSubmission2","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:20:03.986791Z","iopub.status.busy":"2024-12-12T20:20:03.986415Z","iopub.status.idle":"2024-12-12T20:22:09.919974Z","shell.execute_reply":"2024-12-12T20:22:09.919024Z"},"papermill":{"duration":125.974049,"end_time":"2024-12-12T20:22:09.922074","exception":false,"start_time":"2024-12-12T20:20:03.948025","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# **》》》Model3**\n---","metadata":{"papermill":{"duration":0.038492,"end_time":"2024-12-12T20:22:10.005701","exception":false,"start_time":"2024-12-12T20:22:09.967209","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    thresholds = KappaOPtimizer.x\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tp_rounded = threshold_Rounder(tpm, thresholds)\n\n    return tp_rounded\n\nimputer = SimpleImputer(strategy='median')\n\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n])\n\nSubmission3 = TrainML(ensemble, test)\nSubmission3 = pd.DataFrame({\n    'id': sample['id'],\n    'sii': Submission3\n})\n\nSubmission3","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:22:10.087275Z","iopub.status.busy":"2024-12-12T20:22:10.086958Z","iopub.status.idle":"2024-12-12T20:25:25.899439Z","shell.execute_reply":"2024-12-12T20:25:25.898622Z"},"papermill":{"duration":195.852934,"end_time":"2024-12-12T20:25:25.901091","exception":false,"start_time":"2024-12-12T20:22:10.048157","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Ensemble","metadata":{"papermill":{"duration":0.037551,"end_time":"2024-12-12T20:25:25.97673","exception":false,"start_time":"2024-12-12T20:25:25.939179","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub1 = Submission1\nsub2 = Submission2\nsub3 = Submission3\n\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\n\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii']\n})\n\ndef majority_vote(row):\n    return row.mode()[0]\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'Final_Submission.csv'\")","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:25:26.052482Z","iopub.status.busy":"2024-12-12T20:25:26.052185Z","iopub.status.idle":"2024-12-12T20:25:26.076662Z","shell.execute_reply":"2024-12-12T20:25:26.075852Z"},"papermill":{"duration":0.064111,"end_time":"2024-12-12T20:25:26.078163","exception":false,"start_time":"2024-12-12T20:25:26.014052","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission","metadata":{"execution":{"iopub.execute_input":"2024-12-12T20:25:26.156024Z","iopub.status.busy":"2024-12-12T20:25:26.155772Z","iopub.status.idle":"2024-12-12T20:25:26.163429Z","shell.execute_reply":"2024-12-12T20:25:26.162612Z"},"papermill":{"duration":0.049067,"end_time":"2024-12-12T20:25:26.165082","exception":false,"start_time":"2024-12-12T20:25:26.116015","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}