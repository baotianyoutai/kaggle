{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":203900450,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":7.594014,"end_time":"2024-10-10T11:58:36.355301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T11:58:28.761287","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Note**  \n**Training Notebook cannot run at kaggle platform (because many memory is requied to run).**  \n**If you want to execute this code, you need to prepare own computations (out of kaggle).**  \n\n# Baseline notebooks:\n- Preprocessing : https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags\n- Training (Code only) : **this notebook** https://www.kaggle.com/code/motono0223/js24-train-gbdt-model-with-lags-singlemodel\n  - trained model : https://www.kaggle.com/datasets/motono0223/js24-trained-gbdt-model\n- Inference : https://www.kaggle.com/code/motono0223/js24-inference-gbdt-with-lags-singlemodel\n- EDA(1) : https://www.kaggle.com/code/motono0223/eda-jane-street-real-time-market-data-forecasting\n- EDA(2) : https://www.kaggle.com/code/motono0223/eda-v2-jane-street-real-time-market-forecasting","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport os\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt\nimport pickle\n\nfrom sklearn.metrics import r2_score\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport kaggle_evaluation.jane_street_inference_server","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:24:33.366752Z","iopub.execute_input":"2024-12-22T18:24:33.367175Z","iopub.status.idle":"2024-12-22T18:24:38.404833Z","shell.execute_reply.started":"2024-12-22T18:24:33.367137Z","shell.execute_reply":"2024-12-22T18:24:38.403409Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    seed = 42\n    target_col = \"responder_6\"\n    feature_cols = [\"symbol_id\", \"time_id\"] \\\n        + [f\"feature_{idx:02d}\" for idx in range(79)] \\\n        + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n    categorical_cols = []","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:24:38.407159Z","iopub.execute_input":"2024-12-22T18:24:38.407666Z","iopub.status.idle":"2024-12-22T18:24:38.413455Z","shell.execute_reply.started":"2024-12-22T18:24:38.407630Z","shell.execute_reply":"2024-12-22T18:24:38.412098Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/training.parquet\").collect().to_pandas()\nvalid = pl.scan_parquet(\"/kaggle/input/js24-preprocessing-create-lags/validation.parquet\").collect().to_pandas()\ntrain.shape, valid.shape","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:24:38.414830Z","iopub.execute_input":"2024-12-22T18:24:38.415146Z","iopub.status.idle":"2024-12-22T18:25:22.346860Z","shell.execute_reply.started":"2024-12-22T18:24:38.415116Z","shell.execute_reply":"2024-12-22T18:25:22.345637Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((21022056, 104), (1082224, 104))"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"scan_parquet はデータをすぐにメモリに読み込むのではなく、後で処理するためのクエリプランを作成します（遅延評価）。\n\n.collect(): 遅延評価を実行し、実際にデータをメモリに読み込みます。これによって、Polars の DataFrame が作成されます。\n\n.to_pandas(): Polars の DataFrame を Pandas の DataFrame に変換します。これは、多くの機械学習ライブラリが Pandas の DataFrame を入力として受け取るためによく行われる処理です。","metadata":{}},{"cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:36:26.135095Z","iopub.execute_input":"2024-12-22T18:36:26.136377Z","iopub.status.idle":"2024-12-22T18:36:26.230501Z","shell.execute_reply.started":"2024-12-22T18:36:26.136322Z","shell.execute_reply":"2024-12-22T18:36:26.229578Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                id  date_id  time_id  symbol_id    weight  feature_00  \\\n0         25023058     1101        0          0  1.913215    0.458568   \n1         25023059     1101        0          1  4.360143    0.111398   \n2         25023060     1101        0          2  1.259379    0.536855   \n3         25023061     1101        0          3  1.498643   -0.016845   \n4         25023062     1101        0          4  1.013641    0.269194   \n...            ...      ...      ...        ...       ...         ...   \n21022051  46045109     1669      967         34  2.722149    0.113540   \n21022052  46045110     1669      967         35  1.082290    0.679088   \n21022053  46045111     1669      967         36  1.329738    0.042559   \n21022054  46045112     1669      967         37  1.546024    0.014789   \n21022055  46045113     1669      967         38  2.769701    0.176742   \n\n          feature_01  feature_02  feature_03  feature_04  feature_05  \\\n0          -0.052640   -0.044876    0.124223    2.819707   -0.746395   \n1           0.500473    0.083353    0.395555    2.468412   -0.706918   \n2           0.309993    0.015098    0.318689    2.594687   -1.157400   \n3          -0.104391   -0.202445    0.404921    2.384175   -0.447801   \n4          -0.088913   -0.436358   -0.025382    2.440446   -0.579610   \n...              ...         ...         ...         ...         ...   \n21022051   -1.506321    0.287889    1.114241   -0.619048   -2.316985   \n21022052   -1.118741    0.437552    0.521156   -1.082304   -1.589045   \n21022053   -1.748191    0.211597    0.713040   -0.983708   -1.405621   \n21022054   -1.961609    0.352037    0.816399   -0.905738   -1.192283   \n21022055   -1.467811    0.822198   -0.181564   -0.807722   -1.838525   \n\n          feature_06  feature_07  feature_08  feature_09  feature_10  \\\n0           0.270162    0.232489    0.638483          11           7   \n1           0.313437    0.172877    0.371777          11           7   \n2           0.485360    0.329561    0.671811          81           2   \n3           0.226170    0.172059    0.363244           4           3   \n4           0.207809    0.140545    0.341923          15           1   \n...              ...         ...         ...         ...         ...   \n21022051   -0.342713   -1.570461   -0.053704          42           5   \n21022052   -0.256223   -1.306004   -0.123386          25           7   \n21022053   -0.227169   -1.255891   -0.110412          49           7   \n21022054   -0.327741   -1.812864   -0.193394          34           4   \n21022055   -0.297598   -1.095958   -0.132146          50           1   \n\n          feature_11  feature_12  feature_13  feature_14  feature_15  \\\n0                 76   -0.859027    0.695877   -0.356197         NaN   \n1                 76   -0.878114    1.199868   -0.304281         NaN   \n2                 59   -1.163686   -0.307989   -0.858999         NaN   \n3                 11   -0.915372    1.030181   -0.212352         NaN   \n4                  9   -1.112300    0.355613   -0.510269         NaN   \n...              ...         ...         ...         ...         ...   \n21022051         150    0.112611   -0.177417    0.178268   -0.735336   \n21022052         195   -0.390751    0.058826    0.028477   -0.510665   \n21022053         297   -0.428229   -0.280789   -0.269204   -0.583605   \n21022054         214   -0.185520    0.354104    0.091549   -0.642485   \n21022055         522    0.619401    0.096979    0.549759   -0.763634   \n\n          feature_16  feature_17  feature_18  feature_19  feature_20  \\\n0           0.300569         NaN   -1.081869   -1.249594    0.586737   \n1          -0.151362         NaN   -1.775065   -1.312861    0.167220   \n2          -0.307050         NaN   -1.227922   -1.326199   -1.473596   \n3          -0.169328         NaN   -1.573598   -1.306565   -0.405128   \n4          -0.374868         NaN   -1.360212   -1.567855    1.298702   \n...              ...         ...         ...         ...         ...   \n21022051   -0.331745   -0.633386    0.249368    1.653411    0.002760   \n21022052   -0.516720   -0.353318    1.854869    0.714701    0.542222   \n21022053   -0.643651   -0.659119   -0.163712    2.517011   -0.108909   \n21022054   -0.346708   -0.610686   -0.024380    2.603370   -0.580293   \n21022055   -0.458676   -0.919894   -2.847622    2.650321   -0.346685   \n\n          feature_21  feature_22  feature_23  feature_24  feature_25  \\\n0          -0.283532    0.351779    1.073560   -0.974062   -0.908026   \n1           0.034493    2.383581    1.702920   -0.743340   -0.219267   \n2          -0.266024   -0.474967   -0.199162   -1.760276   -1.485319   \n3           0.050558   -0.176753   -0.542888    0.304873    1.171678   \n4          -0.087483   -0.720150   -0.582005   -0.284471    0.609314   \n...              ...         ...         ...         ...         ...   \n21022051   -0.257053    1.166857    1.184520   -0.362363   -0.943673   \n21022052   -0.084686   -0.630074    0.033593    2.084893    1.046571   \n21022053         NaN   -0.897164   -0.300418    1.143957    0.205031   \n21022054   -0.242773   -0.421886   -0.214082    0.396354   -0.958461   \n21022055   -0.109926    1.264114    0.813752    0.745669   -0.173604   \n\n          feature_26  feature_27  feature_28  feature_29  feature_30  \\\n0           0.737549    0.875670    0.616378    0.052371    0.482981   \n1           0.377675    1.223783    1.276546   -0.343797   -0.154821   \n2           0.677112    1.269566    1.747018   -0.725915   -0.495244   \n3          -0.502733   -1.080110   -0.591162   -0.527882   -0.743914   \n4          -1.606182   -1.231077   -0.251321   -0.552514   -0.454470   \n...              ...         ...         ...         ...         ...   \n21022051   -0.176498    1.246821    1.283275   -0.558301   -0.840702   \n21022052    0.109437   -1.286669   -1.625050   -0.216285   -0.572598   \n21022053         NaN         NaN   -1.477263   -0.447969   -0.503594   \n21022054    0.519482   -0.455202   -0.668232   -0.597619   -0.604933   \n21022055    0.394851    0.597030    0.015387   -0.713974   -0.729069   \n\n          feature_31  feature_32  feature_33  feature_34  feature_35  \\\n0          -0.269024         NaN         NaN   -0.101967   -0.305440   \n1           0.051061         NaN         NaN   -0.533578   -0.531406   \n2          -0.274749         NaN         NaN   -0.027293   -0.618841   \n3           0.045586         NaN         NaN   -0.380765   -0.613219   \n4          -0.085228         NaN         NaN   -0.058304   -0.299192   \n...              ...         ...         ...         ...         ...   \n21022051   -0.292120    0.863269    0.248571    1.312299    1.214976   \n21022052   -0.099471    1.598203    0.740797    0.489917    1.509605   \n21022053         NaN    1.142567   -0.250004    1.226455    1.423985   \n21022054   -0.119973    1.292358    0.201279    1.039528    1.138293   \n21022055   -0.117773    1.295936   -0.517270    1.337012    0.846260   \n\n          feature_36  feature_37  feature_38  feature_39  feature_40  \\\n0           0.745993    0.124049    0.335976         NaN    0.019273   \n1           1.022045   -0.023691    0.231311         NaN    0.783220   \n2           0.878567    0.034158    0.406216         NaN    0.084095   \n3          -0.657111    0.168769    0.181173         NaN   -0.130411   \n4          -0.811605    0.003317    0.107506         NaN    1.561308   \n...              ...         ...         ...         ...         ...   \n21022051    0.421241   -0.019187   -0.135242    0.143081   -0.533547   \n21022052    1.197052    0.320810    0.358670   -0.301272    0.516723   \n21022053    0.218755   -0.629031   -0.287319    0.101322   -0.539646   \n21022054    0.111943   -0.142736   -0.346362    0.119696    0.640974   \n21022055   -1.336178   -0.985287   -1.318583   -0.209264   -0.244748   \n\n          feature_41  feature_42  feature_43  feature_44  feature_45  \\\n0                NaN         NaN   -0.228946         NaN   -1.508707   \n1                NaN         NaN   -0.969126         NaN   -1.442822   \n2                NaN         NaN   -1.285599         NaN   -1.227116   \n3                NaN         NaN   -0.123482         NaN   -1.563487   \n4                NaN         NaN    0.232014         NaN   -1.199686   \n...              ...         ...         ...         ...         ...   \n21022051   -0.740368   -1.387800   -0.644368   -1.282065   -0.574014   \n21022052   -0.236552   -0.350861   -0.223722   -0.690084    0.207996   \n21022053    0.331670   -0.912853   -0.586665   -0.166528    0.837126   \n21022054    0.646920   -0.360532    0.095113    0.170844    1.095095   \n21022055    0.468189   -0.375609   -0.449057   -0.367475    0.443066   \n\n          feature_46  feature_47  feature_48  feature_49  feature_50  \\\n0           0.995993   -1.614079   -0.025769    0.214689         NaN   \n1           1.633847   -0.647865    0.110672    0.106416         NaN   \n2           0.667309   -0.322078    0.078113    0.506323         NaN   \n3           1.488748   -0.309672   -0.302202   -0.510643         NaN   \n4           2.076941   -0.112343    0.649310    0.088501         NaN   \n...              ...         ...         ...         ...         ...   \n21022051   -1.390933   -1.706069   -0.821059   -0.472650    0.222319   \n21022052   -1.021887   -0.417823   -0.402063   -0.356577    0.331667   \n21022053    0.581024   -0.476562   -0.625803   -0.334692   -0.595127   \n21022054    0.806499    0.497883    0.399707    0.419843    0.136760   \n21022055   -0.137041   -0.574692    0.314514   -0.519814   -0.571688   \n\n          feature_51  feature_52  feature_53  feature_54  feature_55  \\\n0           1.344184         NaN         NaN   -1.083561         NaN   \n1           1.489494         NaN         NaN   -0.510090         NaN   \n2          -0.006689         NaN         NaN    0.489376         NaN   \n3           0.974320         NaN         NaN   -1.226123         NaN   \n4           1.103760         NaN         NaN   -0.130490         NaN   \n...              ...         ...         ...         ...         ...   \n21022051   -0.854981   -1.732906   -1.260130   -0.744133   -1.691536   \n21022052   -0.356391   -1.630592   -0.720273   -0.181455   -1.650029   \n21022053   -0.541823    0.615860   -1.346922   -0.072313   -0.407001   \n21022054    0.073017    1.243543   -0.147735   -0.159892   -1.014315   \n21022055    0.086809   -0.048293   -0.800866   -0.253087   -0.194572   \n\n          feature_56  feature_57  feature_58  feature_59  feature_60  \\\n0          -1.226218    1.564233         NaN    0.315116    0.102745   \n1          -0.935670    1.509301         NaN    0.675541    0.269578   \n2          -1.457590    2.773031         NaN    0.681339    0.411752   \n3          -1.674485    1.450467         NaN   -0.444937   -0.380092   \n4          -1.561980    2.156273         NaN    0.869663    0.379331   \n...              ...         ...         ...         ...         ...   \n21022051   -0.865361   -1.340579   -1.953854   -0.867560   -0.797192   \n21022052   -0.322848   -0.791864   -0.512363   -0.901795   -0.422817   \n21022053    0.706010   -0.331250   -0.527262   -0.200182   -0.528069   \n21022054    1.239644    0.397555    0.480661    0.121535    0.399882   \n21022055    1.747492    0.431749   -0.736175    0.040388   -0.425000   \n\n          feature_61  feature_62  feature_63  feature_64  feature_65  \\\n0           0.659961   -0.140479    0.042491   -0.028724   -1.427209   \n1           0.659961   -0.277945   -0.191327   -0.312494   -1.107761   \n2           0.659961   -0.280476   -0.211912   -0.219207   -2.506536   \n3           0.659961   -0.311750   -0.345839   -0.413218   -1.890556   \n4           0.659961   -0.125040   -0.133995   -0.144080   -1.557398   \n...              ...         ...         ...         ...         ...   \n21022051    0.845812   -0.491741   -0.350290   -0.271275    0.192680   \n21022052    0.845812   -0.362909   -0.128486   -0.270118    1.535586   \n21022053    0.845812   -0.498291   -0.387475   -0.217359    0.007208   \n21022054    0.845812   -0.423643   -0.195530   -0.316500   -0.407634   \n21022055    0.845812   -0.378500   -0.415880   -0.425798   -0.553193   \n\n          feature_66  feature_67  feature_68  feature_69  feature_70  \\\n0          -1.993070   -0.862814    1.568264   -0.094396   -0.809437   \n1          -1.085603   -0.759455    1.070539   -0.126439   -1.209990   \n2          -2.055502   -1.226109   -0.096572   -0.445255   -1.033872   \n3          -2.079406   -0.722361    0.424735   -0.286880   -0.814299   \n4          -1.912102   -0.894412    0.344008   -0.377989   -1.108163   \n...              ...         ...         ...         ...         ...   \n21022051    1.365110    0.433228   -0.092753    0.314330   -0.202983   \n21022052    0.542281   -0.224526    0.312888    0.119974   -0.236240   \n21022053    2.756776   -0.305334   -0.240161   -0.063571   -0.451455   \n21022054    2.309432   -0.190548    0.265017    0.154364   -0.131415   \n21022055    2.021812    0.679037    0.074622    0.568490    0.315053   \n\n          feature_71  feature_72  feature_73  feature_74  feature_75  \\\n0          -0.145835   -0.684791         NaN         NaN   -0.122062   \n1           0.309990   -0.537283         NaN         NaN   -0.223740   \n2          -0.290695   -0.800315         NaN         NaN   -0.104352   \n3           1.873171   -0.061714         NaN         NaN    1.163440   \n4           0.061968   -0.652792         NaN         NaN    0.007712   \n...              ...         ...         ...         ...         ...   \n21022051   -0.179797   -0.053907   -0.299687   -0.202872   -0.002706   \n21022052   -0.220552   -0.052969    0.079494    0.016070    0.371137   \n21022053   -0.230157   -0.356269   -0.041393    0.034814   -0.258361   \n21022054    0.309319    0.007948    0.431053    0.322892    1.565560   \n21022055    0.072298    0.196872   -0.264101   -0.061993    1.045759   \n\n          feature_76  feature_77  feature_78  responder_0  responder_1  \\\n0          -0.378493   -0.364320   -0.260619     0.108411    -0.189889   \n1          -0.256861   -0.288086   -0.323778    -0.155402    -0.154787   \n2          -0.221018   -0.382566   -0.330158    -0.327308    -0.253688   \n3           1.120400    0.125606    0.188438    -0.002188    -0.102668   \n4           0.001405   -0.245694   -0.162255    -0.042635    -0.071551   \n...              ...         ...         ...          ...          ...   \n21022051    0.006981   -0.075956   -0.061255    -0.507107    -0.322154   \n21022052    0.331724    0.088955    0.095219     3.741010     2.473987   \n21022053   -0.171003    0.043774    0.126688     0.219835     0.039012   \n21022054    2.066381    0.710083    0.698162    -0.244743    -0.689590   \n21022055    1.517300    0.202212    0.444961     0.765402    -0.095149   \n\n          responder_2  responder_3  responder_4  responder_5  responder_6  \\\n0            0.095152     1.188086     0.183916    -0.569924     1.233719   \n1           -0.243248    -0.497277    -0.258040    -0.358548    -0.582184   \n2           -0.561014    -1.515344     1.919260    -3.469981    -1.465263   \n3           -0.763048    -0.654262    -1.305771    -0.187887    -0.793223   \n4           -0.114196    -0.278691    -0.149025    -0.661970    -0.257424   \n...               ...          ...          ...          ...          ...   \n21022051     0.990749    -0.036050    -0.016980     0.308726    -0.053505   \n21022052    -0.392999     0.827562     0.546331    -0.453335     0.026022   \n21022053     0.836770     0.048242     0.034382     0.196874    -0.387762   \n21022054     0.229692     0.970275     0.586525     1.495717     0.664569   \n21022055     0.444877     0.507986     0.095263     0.277648    -0.151708   \n\n          responder_7  responder_8  partition_id  label  responder_0_lag_1  \\\n0            0.429217    -0.708557             6      2                NaN   \n1           -0.333726    -0.268150             6     -1                NaN   \n2            1.964456    -3.273642             6     -2                NaN   \n3           -1.376819     0.201750             6     -1                NaN   \n4           -0.088133    -1.544584             6      0                NaN   \n...               ...          ...           ...    ...                ...   \n21022051     0.002701    -0.098526             9      0          -0.075389   \n21022052     0.030324     0.024453             9      0          -1.687717   \n21022053    -0.284657    -1.036527             9      0           2.412394   \n21022054     0.512183     1.819349             9      1          -0.367726   \n21022055    -0.042970    -0.272604             9      0          -1.431064   \n\n          responder_1_lag_1  responder_2_lag_1  responder_3_lag_1  \\\n0                       NaN                NaN                NaN   \n1                       NaN                NaN                NaN   \n2                       NaN                NaN                NaN   \n3                       NaN                NaN                NaN   \n4                       NaN                NaN                NaN   \n...                     ...                ...                ...   \n21022051          -0.047733          -0.230586           0.145945   \n21022052          -1.179804          -0.036565           0.390897   \n21022053           1.328863           2.740413          -0.428463   \n21022054          -0.190907          -0.104239          -0.081754   \n21022055          -0.925963          -0.972953          -0.236310   \n\n          responder_4_lag_1  responder_5_lag_1  responder_6_lag_1  \\\n0                       NaN                NaN                NaN   \n1                       NaN                NaN                NaN   \n2                       NaN                NaN                NaN   \n3                       NaN                NaN                NaN   \n4                       NaN                NaN                NaN   \n...                     ...                ...                ...   \n21022051           0.104809          -0.177008          -0.023749   \n21022052           0.165945          -0.377233          -0.555132   \n21022053          -0.223387           0.746468           0.143096   \n21022054          -0.026687          -0.168766          -0.207993   \n21022055          -0.145933          -0.433827          -0.185205   \n\n          responder_7_lag_1  responder_8_lag_1  \n0                       NaN                NaN  \n1                       NaN                NaN  \n2                       NaN                NaN  \n3                       NaN                NaN  \n4                       NaN                NaN  \n...                     ...                ...  \n21022051           0.022756          -0.076810  \n21022052          -0.132948          -1.025781  \n21022053           0.080777           0.218555  \n21022054          -0.083194          -0.592887  \n21022055          -0.071732          -0.409495  \n\n[21022056 rows x 104 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date_id</th>\n      <th>time_id</th>\n      <th>symbol_id</th>\n      <th>weight</th>\n      <th>feature_00</th>\n      <th>feature_01</th>\n      <th>feature_02</th>\n      <th>feature_03</th>\n      <th>feature_04</th>\n      <th>feature_05</th>\n      <th>feature_06</th>\n      <th>feature_07</th>\n      <th>feature_08</th>\n      <th>feature_09</th>\n      <th>feature_10</th>\n      <th>feature_11</th>\n      <th>feature_12</th>\n      <th>feature_13</th>\n      <th>feature_14</th>\n      <th>feature_15</th>\n      <th>feature_16</th>\n      <th>feature_17</th>\n      <th>feature_18</th>\n      <th>feature_19</th>\n      <th>feature_20</th>\n      <th>feature_21</th>\n      <th>feature_22</th>\n      <th>feature_23</th>\n      <th>feature_24</th>\n      <th>feature_25</th>\n      <th>feature_26</th>\n      <th>feature_27</th>\n      <th>feature_28</th>\n      <th>feature_29</th>\n      <th>feature_30</th>\n      <th>feature_31</th>\n      <th>feature_32</th>\n      <th>feature_33</th>\n      <th>feature_34</th>\n      <th>feature_35</th>\n      <th>feature_36</th>\n      <th>feature_37</th>\n      <th>feature_38</th>\n      <th>feature_39</th>\n      <th>feature_40</th>\n      <th>feature_41</th>\n      <th>feature_42</th>\n      <th>feature_43</th>\n      <th>feature_44</th>\n      <th>feature_45</th>\n      <th>feature_46</th>\n      <th>feature_47</th>\n      <th>feature_48</th>\n      <th>feature_49</th>\n      <th>feature_50</th>\n      <th>feature_51</th>\n      <th>feature_52</th>\n      <th>feature_53</th>\n      <th>feature_54</th>\n      <th>feature_55</th>\n      <th>feature_56</th>\n      <th>feature_57</th>\n      <th>feature_58</th>\n      <th>feature_59</th>\n      <th>feature_60</th>\n      <th>feature_61</th>\n      <th>feature_62</th>\n      <th>feature_63</th>\n      <th>feature_64</th>\n      <th>feature_65</th>\n      <th>feature_66</th>\n      <th>feature_67</th>\n      <th>feature_68</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n      <th>feature_75</th>\n      <th>feature_76</th>\n      <th>feature_77</th>\n      <th>feature_78</th>\n      <th>responder_0</th>\n      <th>responder_1</th>\n      <th>responder_2</th>\n      <th>responder_3</th>\n      <th>responder_4</th>\n      <th>responder_5</th>\n      <th>responder_6</th>\n      <th>responder_7</th>\n      <th>responder_8</th>\n      <th>partition_id</th>\n      <th>label</th>\n      <th>responder_0_lag_1</th>\n      <th>responder_1_lag_1</th>\n      <th>responder_2_lag_1</th>\n      <th>responder_3_lag_1</th>\n      <th>responder_4_lag_1</th>\n      <th>responder_5_lag_1</th>\n      <th>responder_6_lag_1</th>\n      <th>responder_7_lag_1</th>\n      <th>responder_8_lag_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25023058</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.913215</td>\n      <td>0.458568</td>\n      <td>-0.052640</td>\n      <td>-0.044876</td>\n      <td>0.124223</td>\n      <td>2.819707</td>\n      <td>-0.746395</td>\n      <td>0.270162</td>\n      <td>0.232489</td>\n      <td>0.638483</td>\n      <td>11</td>\n      <td>7</td>\n      <td>76</td>\n      <td>-0.859027</td>\n      <td>0.695877</td>\n      <td>-0.356197</td>\n      <td>NaN</td>\n      <td>0.300569</td>\n      <td>NaN</td>\n      <td>-1.081869</td>\n      <td>-1.249594</td>\n      <td>0.586737</td>\n      <td>-0.283532</td>\n      <td>0.351779</td>\n      <td>1.073560</td>\n      <td>-0.974062</td>\n      <td>-0.908026</td>\n      <td>0.737549</td>\n      <td>0.875670</td>\n      <td>0.616378</td>\n      <td>0.052371</td>\n      <td>0.482981</td>\n      <td>-0.269024</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.101967</td>\n      <td>-0.305440</td>\n      <td>0.745993</td>\n      <td>0.124049</td>\n      <td>0.335976</td>\n      <td>NaN</td>\n      <td>0.019273</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.228946</td>\n      <td>NaN</td>\n      <td>-1.508707</td>\n      <td>0.995993</td>\n      <td>-1.614079</td>\n      <td>-0.025769</td>\n      <td>0.214689</td>\n      <td>NaN</td>\n      <td>1.344184</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.083561</td>\n      <td>NaN</td>\n      <td>-1.226218</td>\n      <td>1.564233</td>\n      <td>NaN</td>\n      <td>0.315116</td>\n      <td>0.102745</td>\n      <td>0.659961</td>\n      <td>-0.140479</td>\n      <td>0.042491</td>\n      <td>-0.028724</td>\n      <td>-1.427209</td>\n      <td>-1.993070</td>\n      <td>-0.862814</td>\n      <td>1.568264</td>\n      <td>-0.094396</td>\n      <td>-0.809437</td>\n      <td>-0.145835</td>\n      <td>-0.684791</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.122062</td>\n      <td>-0.378493</td>\n      <td>-0.364320</td>\n      <td>-0.260619</td>\n      <td>0.108411</td>\n      <td>-0.189889</td>\n      <td>0.095152</td>\n      <td>1.188086</td>\n      <td>0.183916</td>\n      <td>-0.569924</td>\n      <td>1.233719</td>\n      <td>0.429217</td>\n      <td>-0.708557</td>\n      <td>6</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25023059</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4.360143</td>\n      <td>0.111398</td>\n      <td>0.500473</td>\n      <td>0.083353</td>\n      <td>0.395555</td>\n      <td>2.468412</td>\n      <td>-0.706918</td>\n      <td>0.313437</td>\n      <td>0.172877</td>\n      <td>0.371777</td>\n      <td>11</td>\n      <td>7</td>\n      <td>76</td>\n      <td>-0.878114</td>\n      <td>1.199868</td>\n      <td>-0.304281</td>\n      <td>NaN</td>\n      <td>-0.151362</td>\n      <td>NaN</td>\n      <td>-1.775065</td>\n      <td>-1.312861</td>\n      <td>0.167220</td>\n      <td>0.034493</td>\n      <td>2.383581</td>\n      <td>1.702920</td>\n      <td>-0.743340</td>\n      <td>-0.219267</td>\n      <td>0.377675</td>\n      <td>1.223783</td>\n      <td>1.276546</td>\n      <td>-0.343797</td>\n      <td>-0.154821</td>\n      <td>0.051061</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.533578</td>\n      <td>-0.531406</td>\n      <td>1.022045</td>\n      <td>-0.023691</td>\n      <td>0.231311</td>\n      <td>NaN</td>\n      <td>0.783220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.969126</td>\n      <td>NaN</td>\n      <td>-1.442822</td>\n      <td>1.633847</td>\n      <td>-0.647865</td>\n      <td>0.110672</td>\n      <td>0.106416</td>\n      <td>NaN</td>\n      <td>1.489494</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.510090</td>\n      <td>NaN</td>\n      <td>-0.935670</td>\n      <td>1.509301</td>\n      <td>NaN</td>\n      <td>0.675541</td>\n      <td>0.269578</td>\n      <td>0.659961</td>\n      <td>-0.277945</td>\n      <td>-0.191327</td>\n      <td>-0.312494</td>\n      <td>-1.107761</td>\n      <td>-1.085603</td>\n      <td>-0.759455</td>\n      <td>1.070539</td>\n      <td>-0.126439</td>\n      <td>-1.209990</td>\n      <td>0.309990</td>\n      <td>-0.537283</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.223740</td>\n      <td>-0.256861</td>\n      <td>-0.288086</td>\n      <td>-0.323778</td>\n      <td>-0.155402</td>\n      <td>-0.154787</td>\n      <td>-0.243248</td>\n      <td>-0.497277</td>\n      <td>-0.258040</td>\n      <td>-0.358548</td>\n      <td>-0.582184</td>\n      <td>-0.333726</td>\n      <td>-0.268150</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25023060</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.259379</td>\n      <td>0.536855</td>\n      <td>0.309993</td>\n      <td>0.015098</td>\n      <td>0.318689</td>\n      <td>2.594687</td>\n      <td>-1.157400</td>\n      <td>0.485360</td>\n      <td>0.329561</td>\n      <td>0.671811</td>\n      <td>81</td>\n      <td>2</td>\n      <td>59</td>\n      <td>-1.163686</td>\n      <td>-0.307989</td>\n      <td>-0.858999</td>\n      <td>NaN</td>\n      <td>-0.307050</td>\n      <td>NaN</td>\n      <td>-1.227922</td>\n      <td>-1.326199</td>\n      <td>-1.473596</td>\n      <td>-0.266024</td>\n      <td>-0.474967</td>\n      <td>-0.199162</td>\n      <td>-1.760276</td>\n      <td>-1.485319</td>\n      <td>0.677112</td>\n      <td>1.269566</td>\n      <td>1.747018</td>\n      <td>-0.725915</td>\n      <td>-0.495244</td>\n      <td>-0.274749</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.027293</td>\n      <td>-0.618841</td>\n      <td>0.878567</td>\n      <td>0.034158</td>\n      <td>0.406216</td>\n      <td>NaN</td>\n      <td>0.084095</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.285599</td>\n      <td>NaN</td>\n      <td>-1.227116</td>\n      <td>0.667309</td>\n      <td>-0.322078</td>\n      <td>0.078113</td>\n      <td>0.506323</td>\n      <td>NaN</td>\n      <td>-0.006689</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.489376</td>\n      <td>NaN</td>\n      <td>-1.457590</td>\n      <td>2.773031</td>\n      <td>NaN</td>\n      <td>0.681339</td>\n      <td>0.411752</td>\n      <td>0.659961</td>\n      <td>-0.280476</td>\n      <td>-0.211912</td>\n      <td>-0.219207</td>\n      <td>-2.506536</td>\n      <td>-2.055502</td>\n      <td>-1.226109</td>\n      <td>-0.096572</td>\n      <td>-0.445255</td>\n      <td>-1.033872</td>\n      <td>-0.290695</td>\n      <td>-0.800315</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.104352</td>\n      <td>-0.221018</td>\n      <td>-0.382566</td>\n      <td>-0.330158</td>\n      <td>-0.327308</td>\n      <td>-0.253688</td>\n      <td>-0.561014</td>\n      <td>-1.515344</td>\n      <td>1.919260</td>\n      <td>-3.469981</td>\n      <td>-1.465263</td>\n      <td>1.964456</td>\n      <td>-3.273642</td>\n      <td>6</td>\n      <td>-2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25023061</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.498643</td>\n      <td>-0.016845</td>\n      <td>-0.104391</td>\n      <td>-0.202445</td>\n      <td>0.404921</td>\n      <td>2.384175</td>\n      <td>-0.447801</td>\n      <td>0.226170</td>\n      <td>0.172059</td>\n      <td>0.363244</td>\n      <td>4</td>\n      <td>3</td>\n      <td>11</td>\n      <td>-0.915372</td>\n      <td>1.030181</td>\n      <td>-0.212352</td>\n      <td>NaN</td>\n      <td>-0.169328</td>\n      <td>NaN</td>\n      <td>-1.573598</td>\n      <td>-1.306565</td>\n      <td>-0.405128</td>\n      <td>0.050558</td>\n      <td>-0.176753</td>\n      <td>-0.542888</td>\n      <td>0.304873</td>\n      <td>1.171678</td>\n      <td>-0.502733</td>\n      <td>-1.080110</td>\n      <td>-0.591162</td>\n      <td>-0.527882</td>\n      <td>-0.743914</td>\n      <td>0.045586</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.380765</td>\n      <td>-0.613219</td>\n      <td>-0.657111</td>\n      <td>0.168769</td>\n      <td>0.181173</td>\n      <td>NaN</td>\n      <td>-0.130411</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.123482</td>\n      <td>NaN</td>\n      <td>-1.563487</td>\n      <td>1.488748</td>\n      <td>-0.309672</td>\n      <td>-0.302202</td>\n      <td>-0.510643</td>\n      <td>NaN</td>\n      <td>0.974320</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.226123</td>\n      <td>NaN</td>\n      <td>-1.674485</td>\n      <td>1.450467</td>\n      <td>NaN</td>\n      <td>-0.444937</td>\n      <td>-0.380092</td>\n      <td>0.659961</td>\n      <td>-0.311750</td>\n      <td>-0.345839</td>\n      <td>-0.413218</td>\n      <td>-1.890556</td>\n      <td>-2.079406</td>\n      <td>-0.722361</td>\n      <td>0.424735</td>\n      <td>-0.286880</td>\n      <td>-0.814299</td>\n      <td>1.873171</td>\n      <td>-0.061714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.163440</td>\n      <td>1.120400</td>\n      <td>0.125606</td>\n      <td>0.188438</td>\n      <td>-0.002188</td>\n      <td>-0.102668</td>\n      <td>-0.763048</td>\n      <td>-0.654262</td>\n      <td>-1.305771</td>\n      <td>-0.187887</td>\n      <td>-0.793223</td>\n      <td>-1.376819</td>\n      <td>0.201750</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25023062</td>\n      <td>1101</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.013641</td>\n      <td>0.269194</td>\n      <td>-0.088913</td>\n      <td>-0.436358</td>\n      <td>-0.025382</td>\n      <td>2.440446</td>\n      <td>-0.579610</td>\n      <td>0.207809</td>\n      <td>0.140545</td>\n      <td>0.341923</td>\n      <td>15</td>\n      <td>1</td>\n      <td>9</td>\n      <td>-1.112300</td>\n      <td>0.355613</td>\n      <td>-0.510269</td>\n      <td>NaN</td>\n      <td>-0.374868</td>\n      <td>NaN</td>\n      <td>-1.360212</td>\n      <td>-1.567855</td>\n      <td>1.298702</td>\n      <td>-0.087483</td>\n      <td>-0.720150</td>\n      <td>-0.582005</td>\n      <td>-0.284471</td>\n      <td>0.609314</td>\n      <td>-1.606182</td>\n      <td>-1.231077</td>\n      <td>-0.251321</td>\n      <td>-0.552514</td>\n      <td>-0.454470</td>\n      <td>-0.085228</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.058304</td>\n      <td>-0.299192</td>\n      <td>-0.811605</td>\n      <td>0.003317</td>\n      <td>0.107506</td>\n      <td>NaN</td>\n      <td>1.561308</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.232014</td>\n      <td>NaN</td>\n      <td>-1.199686</td>\n      <td>2.076941</td>\n      <td>-0.112343</td>\n      <td>0.649310</td>\n      <td>0.088501</td>\n      <td>NaN</td>\n      <td>1.103760</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.130490</td>\n      <td>NaN</td>\n      <td>-1.561980</td>\n      <td>2.156273</td>\n      <td>NaN</td>\n      <td>0.869663</td>\n      <td>0.379331</td>\n      <td>0.659961</td>\n      <td>-0.125040</td>\n      <td>-0.133995</td>\n      <td>-0.144080</td>\n      <td>-1.557398</td>\n      <td>-1.912102</td>\n      <td>-0.894412</td>\n      <td>0.344008</td>\n      <td>-0.377989</td>\n      <td>-1.108163</td>\n      <td>0.061968</td>\n      <td>-0.652792</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.007712</td>\n      <td>0.001405</td>\n      <td>-0.245694</td>\n      <td>-0.162255</td>\n      <td>-0.042635</td>\n      <td>-0.071551</td>\n      <td>-0.114196</td>\n      <td>-0.278691</td>\n      <td>-0.149025</td>\n      <td>-0.661970</td>\n      <td>-0.257424</td>\n      <td>-0.088133</td>\n      <td>-1.544584</td>\n      <td>6</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21022051</th>\n      <td>46045109</td>\n      <td>1669</td>\n      <td>967</td>\n      <td>34</td>\n      <td>2.722149</td>\n      <td>0.113540</td>\n      <td>-1.506321</td>\n      <td>0.287889</td>\n      <td>1.114241</td>\n      <td>-0.619048</td>\n      <td>-2.316985</td>\n      <td>-0.342713</td>\n      <td>-1.570461</td>\n      <td>-0.053704</td>\n      <td>42</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.112611</td>\n      <td>-0.177417</td>\n      <td>0.178268</td>\n      <td>-0.735336</td>\n      <td>-0.331745</td>\n      <td>-0.633386</td>\n      <td>0.249368</td>\n      <td>1.653411</td>\n      <td>0.002760</td>\n      <td>-0.257053</td>\n      <td>1.166857</td>\n      <td>1.184520</td>\n      <td>-0.362363</td>\n      <td>-0.943673</td>\n      <td>-0.176498</td>\n      <td>1.246821</td>\n      <td>1.283275</td>\n      <td>-0.558301</td>\n      <td>-0.840702</td>\n      <td>-0.292120</td>\n      <td>0.863269</td>\n      <td>0.248571</td>\n      <td>1.312299</td>\n      <td>1.214976</td>\n      <td>0.421241</td>\n      <td>-0.019187</td>\n      <td>-0.135242</td>\n      <td>0.143081</td>\n      <td>-0.533547</td>\n      <td>-0.740368</td>\n      <td>-1.387800</td>\n      <td>-0.644368</td>\n      <td>-1.282065</td>\n      <td>-0.574014</td>\n      <td>-1.390933</td>\n      <td>-1.706069</td>\n      <td>-0.821059</td>\n      <td>-0.472650</td>\n      <td>0.222319</td>\n      <td>-0.854981</td>\n      <td>-1.732906</td>\n      <td>-1.260130</td>\n      <td>-0.744133</td>\n      <td>-1.691536</td>\n      <td>-0.865361</td>\n      <td>-1.340579</td>\n      <td>-1.953854</td>\n      <td>-0.867560</td>\n      <td>-0.797192</td>\n      <td>0.845812</td>\n      <td>-0.491741</td>\n      <td>-0.350290</td>\n      <td>-0.271275</td>\n      <td>0.192680</td>\n      <td>1.365110</td>\n      <td>0.433228</td>\n      <td>-0.092753</td>\n      <td>0.314330</td>\n      <td>-0.202983</td>\n      <td>-0.179797</td>\n      <td>-0.053907</td>\n      <td>-0.299687</td>\n      <td>-0.202872</td>\n      <td>-0.002706</td>\n      <td>0.006981</td>\n      <td>-0.075956</td>\n      <td>-0.061255</td>\n      <td>-0.507107</td>\n      <td>-0.322154</td>\n      <td>0.990749</td>\n      <td>-0.036050</td>\n      <td>-0.016980</td>\n      <td>0.308726</td>\n      <td>-0.053505</td>\n      <td>0.002701</td>\n      <td>-0.098526</td>\n      <td>9</td>\n      <td>0</td>\n      <td>-0.075389</td>\n      <td>-0.047733</td>\n      <td>-0.230586</td>\n      <td>0.145945</td>\n      <td>0.104809</td>\n      <td>-0.177008</td>\n      <td>-0.023749</td>\n      <td>0.022756</td>\n      <td>-0.076810</td>\n    </tr>\n    <tr>\n      <th>21022052</th>\n      <td>46045110</td>\n      <td>1669</td>\n      <td>967</td>\n      <td>35</td>\n      <td>1.082290</td>\n      <td>0.679088</td>\n      <td>-1.118741</td>\n      <td>0.437552</td>\n      <td>0.521156</td>\n      <td>-1.082304</td>\n      <td>-1.589045</td>\n      <td>-0.256223</td>\n      <td>-1.306004</td>\n      <td>-0.123386</td>\n      <td>25</td>\n      <td>7</td>\n      <td>195</td>\n      <td>-0.390751</td>\n      <td>0.058826</td>\n      <td>0.028477</td>\n      <td>-0.510665</td>\n      <td>-0.516720</td>\n      <td>-0.353318</td>\n      <td>1.854869</td>\n      <td>0.714701</td>\n      <td>0.542222</td>\n      <td>-0.084686</td>\n      <td>-0.630074</td>\n      <td>0.033593</td>\n      <td>2.084893</td>\n      <td>1.046571</td>\n      <td>0.109437</td>\n      <td>-1.286669</td>\n      <td>-1.625050</td>\n      <td>-0.216285</td>\n      <td>-0.572598</td>\n      <td>-0.099471</td>\n      <td>1.598203</td>\n      <td>0.740797</td>\n      <td>0.489917</td>\n      <td>1.509605</td>\n      <td>1.197052</td>\n      <td>0.320810</td>\n      <td>0.358670</td>\n      <td>-0.301272</td>\n      <td>0.516723</td>\n      <td>-0.236552</td>\n      <td>-0.350861</td>\n      <td>-0.223722</td>\n      <td>-0.690084</td>\n      <td>0.207996</td>\n      <td>-1.021887</td>\n      <td>-0.417823</td>\n      <td>-0.402063</td>\n      <td>-0.356577</td>\n      <td>0.331667</td>\n      <td>-0.356391</td>\n      <td>-1.630592</td>\n      <td>-0.720273</td>\n      <td>-0.181455</td>\n      <td>-1.650029</td>\n      <td>-0.322848</td>\n      <td>-0.791864</td>\n      <td>-0.512363</td>\n      <td>-0.901795</td>\n      <td>-0.422817</td>\n      <td>0.845812</td>\n      <td>-0.362909</td>\n      <td>-0.128486</td>\n      <td>-0.270118</td>\n      <td>1.535586</td>\n      <td>0.542281</td>\n      <td>-0.224526</td>\n      <td>0.312888</td>\n      <td>0.119974</td>\n      <td>-0.236240</td>\n      <td>-0.220552</td>\n      <td>-0.052969</td>\n      <td>0.079494</td>\n      <td>0.016070</td>\n      <td>0.371137</td>\n      <td>0.331724</td>\n      <td>0.088955</td>\n      <td>0.095219</td>\n      <td>3.741010</td>\n      <td>2.473987</td>\n      <td>-0.392999</td>\n      <td>0.827562</td>\n      <td>0.546331</td>\n      <td>-0.453335</td>\n      <td>0.026022</td>\n      <td>0.030324</td>\n      <td>0.024453</td>\n      <td>9</td>\n      <td>0</td>\n      <td>-1.687717</td>\n      <td>-1.179804</td>\n      <td>-0.036565</td>\n      <td>0.390897</td>\n      <td>0.165945</td>\n      <td>-0.377233</td>\n      <td>-0.555132</td>\n      <td>-0.132948</td>\n      <td>-1.025781</td>\n    </tr>\n    <tr>\n      <th>21022053</th>\n      <td>46045111</td>\n      <td>1669</td>\n      <td>967</td>\n      <td>36</td>\n      <td>1.329738</td>\n      <td>0.042559</td>\n      <td>-1.748191</td>\n      <td>0.211597</td>\n      <td>0.713040</td>\n      <td>-0.983708</td>\n      <td>-1.405621</td>\n      <td>-0.227169</td>\n      <td>-1.255891</td>\n      <td>-0.110412</td>\n      <td>49</td>\n      <td>7</td>\n      <td>297</td>\n      <td>-0.428229</td>\n      <td>-0.280789</td>\n      <td>-0.269204</td>\n      <td>-0.583605</td>\n      <td>-0.643651</td>\n      <td>-0.659119</td>\n      <td>-0.163712</td>\n      <td>2.517011</td>\n      <td>-0.108909</td>\n      <td>NaN</td>\n      <td>-0.897164</td>\n      <td>-0.300418</td>\n      <td>1.143957</td>\n      <td>0.205031</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1.477263</td>\n      <td>-0.447969</td>\n      <td>-0.503594</td>\n      <td>NaN</td>\n      <td>1.142567</td>\n      <td>-0.250004</td>\n      <td>1.226455</td>\n      <td>1.423985</td>\n      <td>0.218755</td>\n      <td>-0.629031</td>\n      <td>-0.287319</td>\n      <td>0.101322</td>\n      <td>-0.539646</td>\n      <td>0.331670</td>\n      <td>-0.912853</td>\n      <td>-0.586665</td>\n      <td>-0.166528</td>\n      <td>0.837126</td>\n      <td>0.581024</td>\n      <td>-0.476562</td>\n      <td>-0.625803</td>\n      <td>-0.334692</td>\n      <td>-0.595127</td>\n      <td>-0.541823</td>\n      <td>0.615860</td>\n      <td>-1.346922</td>\n      <td>-0.072313</td>\n      <td>-0.407001</td>\n      <td>0.706010</td>\n      <td>-0.331250</td>\n      <td>-0.527262</td>\n      <td>-0.200182</td>\n      <td>-0.528069</td>\n      <td>0.845812</td>\n      <td>-0.498291</td>\n      <td>-0.387475</td>\n      <td>-0.217359</td>\n      <td>0.007208</td>\n      <td>2.756776</td>\n      <td>-0.305334</td>\n      <td>-0.240161</td>\n      <td>-0.063571</td>\n      <td>-0.451455</td>\n      <td>-0.230157</td>\n      <td>-0.356269</td>\n      <td>-0.041393</td>\n      <td>0.034814</td>\n      <td>-0.258361</td>\n      <td>-0.171003</td>\n      <td>0.043774</td>\n      <td>0.126688</td>\n      <td>0.219835</td>\n      <td>0.039012</td>\n      <td>0.836770</td>\n      <td>0.048242</td>\n      <td>0.034382</td>\n      <td>0.196874</td>\n      <td>-0.387762</td>\n      <td>-0.284657</td>\n      <td>-1.036527</td>\n      <td>9</td>\n      <td>0</td>\n      <td>2.412394</td>\n      <td>1.328863</td>\n      <td>2.740413</td>\n      <td>-0.428463</td>\n      <td>-0.223387</td>\n      <td>0.746468</td>\n      <td>0.143096</td>\n      <td>0.080777</td>\n      <td>0.218555</td>\n    </tr>\n    <tr>\n      <th>21022054</th>\n      <td>46045112</td>\n      <td>1669</td>\n      <td>967</td>\n      <td>37</td>\n      <td>1.546024</td>\n      <td>0.014789</td>\n      <td>-1.961609</td>\n      <td>0.352037</td>\n      <td>0.816399</td>\n      <td>-0.905738</td>\n      <td>-1.192283</td>\n      <td>-0.327741</td>\n      <td>-1.812864</td>\n      <td>-0.193394</td>\n      <td>34</td>\n      <td>4</td>\n      <td>214</td>\n      <td>-0.185520</td>\n      <td>0.354104</td>\n      <td>0.091549</td>\n      <td>-0.642485</td>\n      <td>-0.346708</td>\n      <td>-0.610686</td>\n      <td>-0.024380</td>\n      <td>2.603370</td>\n      <td>-0.580293</td>\n      <td>-0.242773</td>\n      <td>-0.421886</td>\n      <td>-0.214082</td>\n      <td>0.396354</td>\n      <td>-0.958461</td>\n      <td>0.519482</td>\n      <td>-0.455202</td>\n      <td>-0.668232</td>\n      <td>-0.597619</td>\n      <td>-0.604933</td>\n      <td>-0.119973</td>\n      <td>1.292358</td>\n      <td>0.201279</td>\n      <td>1.039528</td>\n      <td>1.138293</td>\n      <td>0.111943</td>\n      <td>-0.142736</td>\n      <td>-0.346362</td>\n      <td>0.119696</td>\n      <td>0.640974</td>\n      <td>0.646920</td>\n      <td>-0.360532</td>\n      <td>0.095113</td>\n      <td>0.170844</td>\n      <td>1.095095</td>\n      <td>0.806499</td>\n      <td>0.497883</td>\n      <td>0.399707</td>\n      <td>0.419843</td>\n      <td>0.136760</td>\n      <td>0.073017</td>\n      <td>1.243543</td>\n      <td>-0.147735</td>\n      <td>-0.159892</td>\n      <td>-1.014315</td>\n      <td>1.239644</td>\n      <td>0.397555</td>\n      <td>0.480661</td>\n      <td>0.121535</td>\n      <td>0.399882</td>\n      <td>0.845812</td>\n      <td>-0.423643</td>\n      <td>-0.195530</td>\n      <td>-0.316500</td>\n      <td>-0.407634</td>\n      <td>2.309432</td>\n      <td>-0.190548</td>\n      <td>0.265017</td>\n      <td>0.154364</td>\n      <td>-0.131415</td>\n      <td>0.309319</td>\n      <td>0.007948</td>\n      <td>0.431053</td>\n      <td>0.322892</td>\n      <td>1.565560</td>\n      <td>2.066381</td>\n      <td>0.710083</td>\n      <td>0.698162</td>\n      <td>-0.244743</td>\n      <td>-0.689590</td>\n      <td>0.229692</td>\n      <td>0.970275</td>\n      <td>0.586525</td>\n      <td>1.495717</td>\n      <td>0.664569</td>\n      <td>0.512183</td>\n      <td>1.819349</td>\n      <td>9</td>\n      <td>1</td>\n      <td>-0.367726</td>\n      <td>-0.190907</td>\n      <td>-0.104239</td>\n      <td>-0.081754</td>\n      <td>-0.026687</td>\n      <td>-0.168766</td>\n      <td>-0.207993</td>\n      <td>-0.083194</td>\n      <td>-0.592887</td>\n    </tr>\n    <tr>\n      <th>21022055</th>\n      <td>46045113</td>\n      <td>1669</td>\n      <td>967</td>\n      <td>38</td>\n      <td>2.769701</td>\n      <td>0.176742</td>\n      <td>-1.467811</td>\n      <td>0.822198</td>\n      <td>-0.181564</td>\n      <td>-0.807722</td>\n      <td>-1.838525</td>\n      <td>-0.297598</td>\n      <td>-1.095958</td>\n      <td>-0.132146</td>\n      <td>50</td>\n      <td>1</td>\n      <td>522</td>\n      <td>0.619401</td>\n      <td>0.096979</td>\n      <td>0.549759</td>\n      <td>-0.763634</td>\n      <td>-0.458676</td>\n      <td>-0.919894</td>\n      <td>-2.847622</td>\n      <td>2.650321</td>\n      <td>-0.346685</td>\n      <td>-0.109926</td>\n      <td>1.264114</td>\n      <td>0.813752</td>\n      <td>0.745669</td>\n      <td>-0.173604</td>\n      <td>0.394851</td>\n      <td>0.597030</td>\n      <td>0.015387</td>\n      <td>-0.713974</td>\n      <td>-0.729069</td>\n      <td>-0.117773</td>\n      <td>1.295936</td>\n      <td>-0.517270</td>\n      <td>1.337012</td>\n      <td>0.846260</td>\n      <td>-1.336178</td>\n      <td>-0.985287</td>\n      <td>-1.318583</td>\n      <td>-0.209264</td>\n      <td>-0.244748</td>\n      <td>0.468189</td>\n      <td>-0.375609</td>\n      <td>-0.449057</td>\n      <td>-0.367475</td>\n      <td>0.443066</td>\n      <td>-0.137041</td>\n      <td>-0.574692</td>\n      <td>0.314514</td>\n      <td>-0.519814</td>\n      <td>-0.571688</td>\n      <td>0.086809</td>\n      <td>-0.048293</td>\n      <td>-0.800866</td>\n      <td>-0.253087</td>\n      <td>-0.194572</td>\n      <td>1.747492</td>\n      <td>0.431749</td>\n      <td>-0.736175</td>\n      <td>0.040388</td>\n      <td>-0.425000</td>\n      <td>0.845812</td>\n      <td>-0.378500</td>\n      <td>-0.415880</td>\n      <td>-0.425798</td>\n      <td>-0.553193</td>\n      <td>2.021812</td>\n      <td>0.679037</td>\n      <td>0.074622</td>\n      <td>0.568490</td>\n      <td>0.315053</td>\n      <td>0.072298</td>\n      <td>0.196872</td>\n      <td>-0.264101</td>\n      <td>-0.061993</td>\n      <td>1.045759</td>\n      <td>1.517300</td>\n      <td>0.202212</td>\n      <td>0.444961</td>\n      <td>0.765402</td>\n      <td>-0.095149</td>\n      <td>0.444877</td>\n      <td>0.507986</td>\n      <td>0.095263</td>\n      <td>0.277648</td>\n      <td>-0.151708</td>\n      <td>-0.042970</td>\n      <td>-0.272604</td>\n      <td>9</td>\n      <td>0</td>\n      <td>-1.431064</td>\n      <td>-0.925963</td>\n      <td>-0.972953</td>\n      <td>-0.236310</td>\n      <td>-0.145933</td>\n      <td>-0.433827</td>\n      <td>-0.185205</td>\n      <td>-0.071732</td>\n      <td>-0.409495</td>\n    </tr>\n  </tbody>\n</table>\n<p>21022056 rows × 104 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Trick of boosting LB score: 0.45->0.49\ntrain = pd.concat([train, valid]).reset_index(drop=True)\ntrain.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T18:40:37.192307Z","iopub.execute_input":"2024-12-22T18:40:37.193270Z","execution_failed":"2024-12-22T18:41:18.694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"トレーニングデータ (train) とバリデーションデータ (valid) を結合し、インデックスをリセットすることで、一つの大きなトレーニングデータを作成\n\npd.concat([train, valid]): Pandas の concat 関数を使用して、train と valid の DataFrame を縦方向に結合します。\n\n[train, valid]：結合する DataFrame のリストです。リストの順に上から下に結合されます。\nデフォルトでは、concat はインデックスを維持します。つまり、元の train と valid のインデックスがそのまま結合後の DataFrame に引き継がれます。\n.reset_index(drop=True): 結合後の DataFrame のインデックスをリセットします。\n\nreset_index(): インデックスをリセットする関数です。\ndrop=True: 元のインデックスを新しい列として追加するのではなく、完全に削除します。これにより、0 から始まる連続した整数の新しいインデックスが作成されます。","metadata":{}},{"cell_type":"markdown","source":"# GBDT models","metadata":{}},{"cell_type":"code","source":"def get_model(seed):\n    # XGBoost parameters\n    XGB_Params = {\n        'learning_rate': 0.05,\n        'max_depth': 6,\n        'n_estimators': 200,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'reg_alpha': 1,\n        'reg_lambda': 5,\n        'random_state': seed,\n        'tree_method': 'gpu_hist',\n        'device' : 'cuda',\n        'n_gpus' : 2,\n    }\n    \n    XGB_Model = XGBRegressor(**XGB_Params)\n    return XGB_Model","metadata":{"execution":{"iopub.status.busy":"2024-12-22T18:44:33.637412Z","iopub.execute_input":"2024-12-22T18:44:33.637964Z","iopub.status.idle":"2024-12-22T18:44:33.670353Z","shell.execute_reply.started":"2024-12-22T18:44:33.637907Z","shell.execute_reply":"2024-12-22T18:44:33.669148Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"このコードは、XGBoost の回帰モデル (XGBRegressor) を生成する関数 get_model(seed) を定義しています。この関数は乱数シード (seed) を引数に取り、特定のパラメータ設定で XGBoost モデルを初期化して返します。","metadata":{}},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"X_train = train[ CONFIG.feature_cols ]\ny_train = train[ CONFIG.target_col ]\nw_train = train[ \"weight\" ]\nX_valid = valid[ CONFIG.feature_cols ]\ny_valid = valid[ CONFIG.target_col ]\nw_valid = valid[ \"weight\" ]\n\nX_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:56:32.41769Z","iopub.execute_input":"2024-10-28T10:56:32.417996Z","iopub.status.idle":"2024-10-28T10:56:36.175344Z","shell.execute_reply.started":"2024-10-28T10:56:32.417964Z","shell.execute_reply":"2024-10-28T10:56:36.174362Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"w_train = train[\"weight\"]: トレーニングデータ train から、重みの列を抽出します。\n\n\"weight\": 重みの列名です。\ntrain[\"weight\"]: train DataFrame から \"weight\" 列を抽出します。抽出されたデータは Pandas の Series として w_train に格納されます。重みは、各データポイントの重要度を表し、損失関数などで使用されます。","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = get_model(CONFIG.seed)\nmodel.fit( X_train, y_train, sample_weight=w_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:56:36.176427Z","iopub.execute_input":"2024-10-28T10:56:36.176729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"このコードは、get_model 関数で生成した XGBoost モデル (model) を、トレーニングデータ (X_train, y_train) と重み (w_train) を使って学習させています。そして、処理にかかった時間を計測しています\n\nsample_weight=w_train: 各トレーニングデータポイントの重みを指定します。w_train は各データポイントに対応する重みの配列です。重みを与えることで、特定のデータポイントを重視した学習を行うことができます。例えば、重要なデータや、誤分類した場合の損失が大きいデータに対して大きな重みを与えることで、モデルの性能を向上させることが期待できます。もしsample_weightを指定しない場合は、全てのデータポイントに対して等しい重みが与えられます。\n","metadata":{}},{"cell_type":"code","source":"y_pred_train1 = model.predict(X_train.iloc[:X_train.shape[0]//2])\ny_pred_train2 = model.predict(X_train.iloc[X_train.shape[0]//2:])\ntrain_score = r2_score(y_train, np.concatenate([y_pred_train1, y_pred_train2], axis=0), sample_weight=w_train )\ntrain_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"このコードは、学習済みモデル (model) を使ってトレーニングデータの予測を行い、その予測結果を評価しています。特に、トレーニングデータを前半と後半に分割して予測を行い、最後にそれらを結合して全体のスコアを計算している点が特徴です。\n\ny_pred_train1 = model.predict(X_train.iloc[:X_train.shape[0]//2]): トレーニングデータ X_train の前半部分を使って予測を行います。\n\nX_train.iloc[:X_train.shape[0]//2]: X_train の最初の行から、X_train の行数の半分までの行を選択します。X_train.shape[0] は X_train の行数を表し、//2 は整数除算（小数点以下切り捨て）を行います。これにより、X_train をほぼ半分に分割します。.iloc は行番号でアクセスするためのインデクサです。\nmodel.predict(...): 学習済みモデル model を使って予測を行います。入力として X_train の前半部分を渡しています。予測結果は y_pred_train1 に格納されます。\n\nnp.concatenate([y_pred_train1, y_pred_train2], axis=0): 前半部分の予測結果 y_pred_train1 と後半部分の予測結果 y_pred_train2 を結合します。axis=0 は縦方向（行方向）に結合することを意味します。これにより、X_train 全体に対する予測結果が得られます。\n\nr2_score(y_train, ..., sample_weight=w_train): R2 スコア（決定係数）を計算します。\ny_train: 正解のターゲット変数です。\nnp.concatenate(...): 予測されたターゲット変数です。\nsample_weight=w_train: 各データポイントの重みを指定します。これにより、重みを考慮した R2 スコアが計算されます。","metadata":{}},{"cell_type":"markdown","source":"このコードの目的:\n\nこのコードの主な目的は、トレーニングデータ全体に対するモデルの性能を評価することです。トレーニングデータを分割して予測を行う理由は、メモリ使用量を抑えるため、または何らかの理由で一度に全てのデータを処理できない場合に有効です。特に大きなデータセットを扱う場合に有用です。\n\nなぜ分割して予測するのか？\n\n大きなデータセットの場合、一度に model.predict(X_train) を実行すると、メモリ不足になる可能性があります。そのため、データを分割して予測を行い、最後に結果を結合することで、メモリ消費を抑えながら予測を行うことができます。","metadata":{}},{"cell_type":"code","source":"y_pred_valid = model.predict(X_valid)\nvalid_score = r2_score(y_valid, y_pred_valid, sample_weight=w_valid )\nvalid_score","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.594977,"end_time":"2024-10-10T11:58:33.569648","exception":false,"start_time":"2024-10-10T11:58:31.974671","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_means = { symbol_id : -1 for symbol_id in range(39) }\nfor symbol_id, gdf in train[[\"symbol_id\", CONFIG.target_col]].groupby(\"symbol_id\"):\n    y_mean = gdf[ CONFIG.target_col ].mean()\n    y_means[symbol_id] = y_mean\n    print(f\"symbol_id = {symbol_id}, y_means = {y_mean:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"このコードは、トレーニングデータ train を symbol_id ごとにグループ化し、各 symbol_id におけるターゲット変数 (CONFIG.target_col) の平均値を計算しています。そして、その結果を辞書 y_means に格納しています。\n\n-----\n\n\ny_means = { symbol_id : -1 for symbol_id in range(39) }: y_means という辞書を初期化しています。\n\n{ symbol_id : -1 for symbol_id in range(39) }: 辞書内包表記を使って、キーが 0 から 38 までの symbol_id、値が -1 の辞書を作成します。これは、各 symbol_id の平均値を格納するための準備で、初期値として -1 を設定しています。もし後で平均値が計算されなかった symbol_id があれば、その値は -1 のままになります。\n\nfor symbol_id, gdf in train[[\"symbol_id\", CONFIG.target_col]].groupby(\"symbol_id\"):: train データフレームを symbol_id でグループ化し、各グループに対して処理を行います。\n\ntrain[[\"symbol_id\", CONFIG.target_col]]: train から symbol_id 列と CONFIG.target_col 列のみを選択します。これにより、必要な列だけを処理することで効率を高めます。\n.groupby(\"symbol_id\"): 選択されたデータフレームを symbol_id でグループ化します。\nfor symbol_id, gdf in ...: グループ化された結果をループで処理します。symbol_id には現在のグループの symbol_id の値が、gdf にはその symbol_id に対応するデータフレーム（グループ）が格納されます。\ny_mean = gdf[CONFIG.target_col].mean(): 現在のグループ (gdf) における CONFIG.target_col の平均値を計算します。\n\ngdf[CONFIG.target_col]: 現在のグループの CONFIG.target_col 列を選択します。\n.mean(): 選択された列の平均値を計算します。\n計算された平均値は y_mean に格納されます。\ny_means[symbol_id] = y_mean: 計算された平均値 y_mean を y_means 辞書に格納します。キーは現在の symbol_id、値は計算された平均値です。\n\nprint(f\"symbol_id = {symbol_id}, y_means = {y_mean:.5f}\"): 現在の symbol_id と計算された平均値を表示します。.5f は小数点以下 5 桁まで表示するフォーマット指定子です。\n\nこのコードの目的:\n\nこのコードの目的は、各 symbol_id ごとのターゲット変数の平均値を計算し、後で利用できるように y_means 辞書に格納することです。この平均値は、例えば予測値の補正や、特徴量として使用するなど、様々な用途に利用できます。\n\nまとめ:\n\nこのコードは、train データフレームを symbol_id でグループ化し、各グループの CONFIG.target_col の平均値を計算して y_means 辞書に格納し、結果を表示します。初期化時に-1を設定することで、データに存在しないsymbol_idを区別できます。","metadata":{}},{"cell_type":"code","source":"cv_detail = { symbol_id : 0 for symbol_id in range(39) }\nfor symbol_id, gdf in valid.groupby(\"symbol_id\"):\n    X_valid = gdf[ CONFIG.feature_cols ]\n    y_valid = gdf[ CONFIG.target_col ]\n    w_valid = gdf[ \"weight\" ]\n    y_pred_valid = model.predict(X_valid)\n    score = r2_score(y_valid, y_pred_valid, sample_weight=w_valid )\n    cv_detail[symbol_id] = score\n    \n    print(f\"symbol_id = {symbol_id}, score = {score:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"このコードは、バリデーションデータ valid を symbol_id ごとにグループ化し、各 symbol_id に対するモデルの性能（R2 スコア）を計算しています。そして、その結果を辞書 cv_detail に格納しています。詳しく解説します。\n\ncv_detail = { symbol_id : 0 for symbol_id in range(39) }: cv_detail という辞書を初期化しています。\n\n{ symbol_id : 0 for symbol_id in range(39) }: 辞書内包表記を使って、キーが 0 から 38 までの symbol_id、値が 0 の辞書を作成します。これは、各 symbol_id のスコアを格納するための準備で、初期値として 0 を設定しています。\nfor symbol_id, gdf in valid.groupby(\"symbol_id\"):: バリデーションデータ valid を symbol_id でグループ化し、各グループに対して処理を行います。\n\nvalid.groupby(\"symbol_id\"): valid DataFrame を symbol_id でグループ化します。\nfor symbol_id, gdf in ...: グループ化された結果をループで処理します。symbol_id には現在のグループの symbol_id の値が、gdf にはその symbol_id に対応するデータフレーム（グループ）が格納されます。\nX_valid = gdf[CONFIG.feature_cols]: 現在のグループ (gdf) から特徴量 (CONFIG.feature_cols) を抽出します。\n\ny_valid = gdf[CONFIG.target_col]: 現在のグループ (gdf) からターゲット変数 (CONFIG.target_col) を抽出します。\n\nw_valid = gdf[\"weight\"]: 現在のグループ (gdf) から重み (\"weight\") を抽出します。\n\ny_pred_valid = model.predict(X_valid): 学習済みモデル (model) を使って、現在のグループのデータ (X_valid) に対する予測を行います。予測結果は y_pred_valid に格納されます。\n\nscore = r2_score(y_valid, y_pred_valid, sample_weight=w_valid): 現在のグループの予測結果を評価し、R2 スコアを計算します。\n\nr2_score(y_valid, y_pred_valid, sample_weight=w_valid): R2 スコアを計算します。sample_weight を指定することで、重みを考慮したスコアが計算されます。\ncv_detail[symbol_id] = score: 計算されたスコア (score) を cv_detail 辞書に格納します。キーは現在の symbol_id です。\n\nprint(f\"symbol_id = {symbol_id}, score = {score:.5f}\"): 現在の symbol_id と計算されたスコアを表示します。.5f は小数点以下 5 桁まで表示するフォーマット指定子です。\n\nこのコードの目的:\n\nこのコードの目的は、各 symbol_id ごとにモデルの性能を評価し、どの symbol_id でモデルの予測が良く、どの symbol_id で悪いのかを把握することです。これにより、モデルの改善点や、特定の symbol_id に特化したモデルの作成などを検討する材料となります。\n\nまとめ:\n\nこのコードは、バリデーションデータを symbol_id でグループ化し、各グループに対して予測を行い、R2 スコアを計算して cv_detail 辞書に格納し、結果を表示します。これにより、symbol_id ごとのモデルの性能を詳細に分析できます。","metadata":{}},{"cell_type":"code","source":"sids = list(cv_detail.keys())\nplt.bar(sids, [cv_detail[sid] for sid in sids])\nplt.grid()\nplt.xlabel(\"symbol_id\")\nplt.ylabel(\"CV score\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"sids = list(cv_detail.keys()): cv_detail 辞書のキー（symbol_id のリスト）を取得し、sids に格納します。\n\ncv_detail.keys(): cv_detail 辞書のキー（symbol_id）のビューオブジェクトを返します。\nlist(...): ビューオブジェクトをリストに変換します。これにより、sids は symbol_id のリストになります。例えば、[0, 1, 2, ..., 38] のようになります。\nplt.bar(sids, [cv_detail[sid] for sid in sids]): 棒グラフを作成します。\n\nplt.bar(x, height): 棒グラフを作成する matplotlib の関数です。\nx: 各棒の x 座標（ここでは symbol_id）。\nheight: 各棒の高さ（ここでは cv_detail の値、つまり CV スコア）。\n[cv_detail[sid] for sid in sids]: リスト内包表記を使って、各 symbol_id に対応する cv_detail の値（CV スコア）のリストを作成します。例えば、[0.5, 0.8, 0.7, ..., 0.6] のようになります。\nplt.grid(): グラフにグリッド線を追加します。これにより、値の読み取りが容易になります。\n\nこのコードの目的:\n\nこのコードの目的は、各 symbol_id におけるモデルの性能（CV スコア）を視覚的に比較することです。棒グラフによって、どの symbol_id でスコアが高く、どの symbol_id でスコアが低いのかが一目でわかります。これにより、モデルの改善点を分析したり、特定の symbol_id に特化した対応を検討したりするのに役立ちます。\n\nまとめ:\n\nこのコードは、cv_detail 辞書に格納された各 symbol_id の CV スコアを棒グラフで可視化します。plt.bar() で棒グラフを作成し、plt.grid() でグリッド線を追加、plt.xlabel() と plt.ylabel() で軸ラベルを設定、plt.show() でグラフを表示します。これにより、各 symbol_id におけるモデルの性能を視覚的に比較することができます。\n","metadata":{}},{"cell_type":"markdown","source":"# Save result","metadata":{}},{"cell_type":"code","source":"result = {\n    \"model\" : model,\n    \"cv\" : valid_score,\n    \"cv_detail\" : cv_detail,\n    \"y_mean\" : y_means,\n}\nwith open(\"result.pkl\", \"wb\") as fp:\n    pickle.dump(result, fp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"このコードは、学習済みモデル (model)、交差検証スコア (valid_score)、交差検証の詳細 (cv_detail)、およびターゲット変数の平均値 (y_means) をまとめて result という辞書に格納し、その辞書を pickle を使ってファイル result.pkl に保存しています。\n\nresult = { ... }: データを格納するための辞書 result を作成します。\n\n\"model\": model: キー \"model\" に学習済みモデル model を格納します。\n\"cv\": valid_score: キー \"cv\" に交差検証スコア valid_score を格納します。\n\"cv_detail\": cv_detail: キー \"cv_detail\" に各 symbol_id の交差検証スコアの詳細 cv_detail を格納します。\n\"y_mean\": y_means: キー \"y_mean\" に各 symbol_id のターゲット変数の平均値 y_means を格納します。\nwith open(\"result.pkl\", \"wb\") as fp:: ファイル result.pkl をバイナリ書き込みモード (\"wb\") で開きます。\n\nwith ... as ...: with 文を使うことで、ファイル操作後のクローズ処理を自動的に行うことができます。これにより、ファイルの閉じ忘れによるエラーを防ぐことができます。\n\"result.pkl\": 保存するファイル名です。拡張子 .pkl は pickle で保存されたファイルであることを示します。\n\"wb\": ファイルをバイナリ書き込みモードで開きます。pickle はオブジェクトをバイナリ形式で保存するため、このモードで開く必要があります。\nas fp: 開いたファイルオブジェクトを fp という変数に割り当てます。\npickle.dump(result, fp): result 辞書をファイル fp に保存します。\n\npickle.dump(obj, file): pickle モジュールの dump 関数を使って、オブジェクト (obj) をファイル (file) にシリアライズして保存します。シリアライズとは、Python オブジェクトをバイトストリームに変換して保存可能な形式にすることです。\nresult: 保存する辞書オブジェクトです。\nfp: 書き込み先のファイルオブジェクトです。\nこのコードの目的:\n\nこのコードの目的は、学習済みモデルや評価結果などの重要なデータをファイルに保存し、後でロードして利用できるようにすることです。特に、学習に時間がかかるモデルの場合、学習済みのモデルを保存しておくことで、再度学習を行う必要がなくなり、時間と計算資源の節約になります。また、評価結果などを保存しておくことで、実験結果の再現や比較が容易になります。\n\npickle とは:\n\npickle は Python の標準ライブラリで、Python オブジェクトをシリアライズ（直列化）およびデシリアライズ（非直列化）するためのモジュールです。シリアライズとは、オブジェクトをバイトストリームに変換して保存可能な形式にすることです。デシリアライズとは、バイトストリームから元のオブジェクトを復元することです。\n\nなぜバイナリモード (\"wb\") で開くのか:\n\npickle はオブジェクトをバイナリ形式で保存するため、ファイルをバイナリモードで開く必要があります。テキストモード (\"w\") で開くと、データが正しく保存されません。\n\nとめ:\n\nこのコードは、学習済みモデル、評価結果などの重要なデータを result 辞書にまとめ、pickle.dump() を使って result.pkl ファイルに保存します。これにより、データを永続化し、後でロードして利用することができます。pickle はオブジェクトをバイナリ形式で保存するため、ファイルはバイナリモードで開く必要があります。","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}